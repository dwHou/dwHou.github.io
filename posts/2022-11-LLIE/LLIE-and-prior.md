### 先验知识

> 广义上讲，给神经网络加入先验，包含约定结果假设，引导神经网络，限制学习路径等。

------

先验知识的种类有很多，最常见的是关于某个/某些特征的单调性、平滑性等。引入这样的先验知识，也可以称为引入单调性、平滑性约束。

这样的约束引入，可以避免因为数据不足或数据分布的不均衡导致的模型拟合出结果违反常识的问题，尽量避免数据采样中自然包含的幸存者偏差，同时也能让模型提供额外的解释性。

#### 单调性约束

单调性约束是在实际场景最为常见的约束之一。比如电商行为预测，要求加购，下单，支付的概率保持单调性。

如果不加入单调性约束，模型当然可以从足够数量的数据中学得这样的单调性特质，并且有更好的MAE。但如果数据不够或者在分布上存在不均衡的问题（实际情况经常如此），那经常就学不到这样的单调性，在业务优化中的可用性就比较有限了。

在模型中加入单调性约束，我们可以在模型设计上想办法，也可以在损失函数/优化上想办法。前者典型的如[Deep Lattice Networks](https://proceedings.neurips.cc/paper/2017/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)，后者典型如引入[Point-wise Loss](https://arxiv.org/pdf/1909.10662.pdf).

#### 平滑性约束

我们经常希望训练出的模型关于某个值的输出结果是平滑函数，这不仅是为了符合业务逻辑，也是为了让模型对对抗攻击更为鲁棒。通常是在损失函数上进行设计，这样的改造意味着广泛的可移植性和极低的改造成本。

#### 非负约束

这可能是我们在不知不觉中使用最广泛的约束，比如softmax、ReLU clamping。

在深度学习中加入先验知识的方法很多，通常需要根据实际场景需求进行个性化的设计，并且常常需要大量的调参过程，偶尔也意味着模型结果（准确率，MAE等）的变差。