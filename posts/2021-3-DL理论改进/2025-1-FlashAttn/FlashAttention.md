# âš¡ Flash Attention (FA) æŠ€æœ¯æ ¸å¿ƒæ€»ç»“

### 1. æ ¸å¿ƒå±æ€§å¯¹æ¯”è¡¨
| ç‰¹æ€§           | åŸç”Ÿ Attention (Eager)       | Flash Attention v1        | Flash Attention v2           |
| :------------- | :--------------------------- | :------------------------ | :--------------------------- |
| **æ•°å­¦ç­‰ä»·æ€§** | åŸºå‡†                         | **å®Œå…¨ç­‰ä»·**              | **å®Œå…¨ç­‰ä»·**                 |
| **æ˜¾å­˜å ç”¨**   | $O(N^2)$ (é«˜)                | $O(N)$ (ä½)               | $O(N)$ (ä½)                  |
| **é€Ÿåº¦è¡¨ç°**   | æ…¢ (IO å—é™)                 | å¿« (åˆ†å—è®¡ç®—)             | æå¿« (å¹¶è¡Œä¼˜åŒ–)              |
| **GPU è¦æ±‚**   | æ— é™åˆ¶                       | Turing (T4/RTX 20) åŠä»¥ä¸Š | Ampere (A100/RTX 30) åŠä»¥ä¸Š  |
| **è®¡ç®—é€»è¾‘**   | å­˜å‚¨å®Œæ•´çš„ $N \times N$ çŸ©é˜µ | SRAM å†…åˆ†å—ï¼Œä¸å­˜å¤§çŸ©é˜µ   | æ”¹è¿›åˆ†å—ç®—æ³•ï¼Œå‡å°‘éçŸ©é˜µè¿ç®— |

---

### 2. PyTorch é›†æˆå†ç¨‹
åœ¨ **PyTorch 2.0** ä¹‹å‰ï¼ŒFlash Attention å¿…é¡»é€šè¿‡ `pip install flash-attn` ç‹¬ç«‹å®‰è£…ã€‚

| PyTorch ç‰ˆæœ¬ | å‘å¸ƒæ—¶é—´  | é›†æˆå†…å®¹                     | æ ¸å¿ƒæ¥å£                         |
| :----------- | :-------- | :--------------------------- | :------------------------------- |
| **< 2.0**    | 2023 ä»¥å‰ | **ä¸è‡ªå¸¦**ï¼Œéœ€æ‰‹åŠ¨ç¼–è¯‘å®‰è£…   | `flash_attn_interface`           |
| **2.0**      | 2023.03   | **é›†æˆ FA v1**               | `F.scaled_dot_product_attention` |
| **2.2**      | 2024.01   | **é›†æˆ FA v2**               | `F.scaled_dot_product_attention` |
| **2.5+**     | 2024.10   | **é›†æˆ FA v3** (Hopper ä¼˜åŒ–) | `F.scaled_dot_product_attention` |

---

### 3. v1 ä¸ v2 çš„è¿›åŒ–å²

#### ğŸŸ¢ Flash Attention v1 (2022) - â€œæ¬è¿å·¥çš„é©å‘½â€
* **ç—›ç‚¹**ï¼šä¼ ç»Ÿçš„ Attention æ…¢ä¸æ˜¯å› ä¸ºè®¡ç®—é‡å¤§ï¼Œæ˜¯å› ä¸ºåœ¨æ˜¾å­˜ï¼ˆHBMï¼‰å’Œç¼“å­˜ï¼ˆSRAMï¼‰ä¹‹é—´åå¤è¯»å†™ $N \times N$ çš„ä¸­é—´çŸ©é˜µï¼Œå¯¼è‡´ I/O ç“¶é¢ˆã€‚
* **æ–¹æ¡ˆ**ï¼š**Tiling (åˆ†å—)**ã€‚å°† $Q, K, V$ æ‹†æˆå°å—æ¬è¿› SRAM ç®—å®Œç›´æ¥å‡ºç»“æœï¼Œä¸å†å†™å›å·¨å¤§çš„ä¸­é—´çŸ©é˜µã€‚

#### ğŸ”µ Flash Attention v2 (2023) - â€œæµæ°´çº¿çš„é‡ç»„â€
* **ç—›ç‚¹**ï¼šv1 è™½è§£å†³äº†å†…å­˜é—®é¢˜ï¼Œä½†åœ¨å¤šå¤„ç†å™¨ï¼ˆSMï¼‰ä¸Šçš„å¹¶è¡Œåˆ†é…ä¸å¤Ÿå¹³å‡ã€‚
* **æ–¹æ¡ˆ**ï¼š
    1.  **å‡å°‘éçŸ©é˜µè¿ç®—**ï¼šå‡å°‘äº† Softmax é‡ç¼©æ”¾æ¬¡æ•°ã€‚
    2.  **å¹¶è¡Œåº¦å¢å¼º**ï¼šæ”¯æŒåœ¨ Sequence Length ç»´åº¦ä¸Šå¹¶è¡Œï¼Œä¸å†å±€é™äº Batch å’Œ Headã€‚
    * **ç»“æœ**ï¼šè®¡ç®—æ•ˆç‡æ¥è¿‘æ˜¾å¡ç†è®ºå³°å€¼ï¼ˆA100 ä¸Šçº¦ 70% çš„ç®—åŠ›åˆ©ç”¨ç‡ï¼‰ã€‚

---

### 4. ç»“è®ºï¼šè·¨æ¶æ„å…¼å®¹æ€§
* **è®­ç»ƒä¸æ¨ç†**ï¼šä½ å¯ä»¥ä½¿ç”¨ Flash Attention v2 è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨æ¨ç†æ—¶å›é€€åˆ°åŸç”Ÿ Attentionã€‚
* **ä¸€è‡´æ€§**ï¼šç»“æœåœ¨æ•°å€¼ä¸Šæå…¶æ¥è¿‘ï¼ˆä»…æœ‰å¾®å°çš„æµ®ç‚¹èˆå…¥è¯¯å·®ï¼‰ï¼Œå¯¹æ¨¡å‹ç²¾åº¦ï¼ˆå¦‚ LLM çš„ç”Ÿæˆè´¨é‡ï¼‰**æ²¡æœ‰å®é™…å½±å“**ã€‚

---

### 5. å¿«é€Ÿæ£€æŸ¥ä»£ç  (PyTorch 2.0+)
```python
import torch
import torch.nn.functional as F

# PyTorch ä¼šæ ¹æ®ä½ çš„æ˜¾å¡è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯
# ä¼˜å…ˆçº§ï¼šFlashAttention > MemoryEfficient > Math(åŸç”Ÿ)
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False):
    output = F.scaled_dot_product_attention(q, k, v)