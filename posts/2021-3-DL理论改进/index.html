<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Traditional Methods</title>

        <link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
        <link rel="stylesheet" href="../../fonts/Serif-Slanted/cmun-serif-slanted.css" />

        <!--BOOTSTRAP-->
        <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
        <!--mobile first-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <!--removed html from url but still is html-->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <!--font awesome-->
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
        <!--fonts: allan & cardo-->
        <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
        <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        <link href="../../css/sticky-footer-navbar.css" rel="stylesheet">

        <link href="../../css/default.css" rel="stylesheet">

        <link href="../../comments/inlineDisqussions.css" rel="stylesheet">

        <!--Highlight-->
        <link href="../../highlight/styles/github.css" rel="stylesheet">

        <link href="../../favicon.ico" rel="shortcut icon" />

        <!--<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
        <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <style>
        .post{width:170px;min-height:175px;padding-left:5px;padding-right:5px;float:left;border-left:1px solid #CCC;background-color:white;}
        div a:first-of-type .post { border-left: none; }
        .post:hover {filter: brightness(90%);}
        .post h3{margin:5px;font-size:75%;text-align:center}
        .post h4{margin:0px;font-size:50%;text-align:center}
        .post img{margin:0px;padding:2px;margin-bottom:10px;width:100%;height:155px}
        </style>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-49811703-1', 'dwHou.github.io');
          ga('require', 'linkid', 'linkid.js');
          ga('require', 'displayfeatures');
          ga('send', 'pageview');

        </script>

    </head>

    <body>
        <div id="wrap">
            <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
                <div class="container">
                    <!--Toggle header for mobile-->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand active" href="../../" style="font-size:20px;">De's blog</a>
                    </div>
                    <!--normal header-->
                    <div class="navbar-collapse collapse">
                        <ul class="nav navbar-nav navbar-right">
                            <li><a href="../../"><span class="glyphicon glyphicon-pencil"></span>  Blog</a></li>
                            <li><a href="../../about.html"><span class="glyphicon glyphicon-user"></span>  About</a></li>
                            <li><a href="../../contact.html"><span class="glyphicon glyphicon-envelope"></span>  Contact</a></li>
                            <li><a href="../../demo.html"><span class="glyphicon glyphicon-play"></span>  ALGO</a></li>
                        </ul>
                    </div><!--/.nav-collapse -->
                </div>
            </nav>


            <div id="content">
                <div class="container">
                    <div class="row">
                        <div class="col-md-8">
                            <h1></h1>
                            <div style="font-size: 170%;">理论前瞻</div>
                            <br>
                            <div class="info">
    <p style="font-family:CMSS; font-size:120%">Posted on Jun.3, 2021</p>

    <!--
        by dwHou
    -->
</div>
</br>

<style>
    ul li {
      margin-top: 12px;
    }

    .tight-list li {
      margin-top: 6px;
    }
  </style>

  在<a href="../2021-3-非深度学习方法">传统方法</a>的前言中，我们提到：
  在性能方面上，DL方法好于非DL方法，在速度上，DL方法有GPU加成可能快于非DL方法。
  我印象中，很多传统方法一直有种想把方法写成GPU版本的倾向。在工业界大数据的加持下，DL方法的鲁棒性也能更好提升。
  传统方法虽具有可解释性，但相对而言吃力不讨好，DL方法则全方面碾压。所以如何利用好DL，是更值得探究的方向。
  然而，距离落地，DL还不是那么完美的，除了工程技巧，一些前沿理论可以助我们打造更好的模型。


  <h2>剪枝</h2>
  <ul>
  <li><b>背景：</b></li>
  <pre></pre>

  </ul>

   
  <h2>嫁接</h2>
  <ul>
  <li><b>出发点</b>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Meng_Filter_Grafting_for_Deep_Neural_Networks_CVPR_2020_paper.pdf"><i class="fa fa-file-pdf-o"></i></a>
  </li>
  <pre>一些失活的卷积限制了网络的潜能。从这个现象出发，<strong><ruby>剪枝<rt>Pruning</rt></ruby></strong>移除了这些失活的卷积，来提升效率。
而<strong><ruby>嫁接<rt>Grafting</rt></ruby></strong>则重新激活它们，来提升性能。</pre>

  <li>tips</li>
  <pre>relu会造成很多梯度始终为0的核，leaky_relu则不会。但leaky_relu同样没法避免出现一些失活的卷积。嫁接这时就能起效果。</pre>
  </ul>

  <h2>权重平均</h2>
  <ul>
  <li><b>思想溯源: </b> 网络插值，从EMA到模型汤</li>
  <pre>
  low-level:
  • 2018年、2019年，ESRGAN的团队同时提出网络插值，用于结合不同优化目标下的模型权重。
  • 2019年、2020年，网络插值被运用在重要赛事中，用于结合不同ckp的权重，旨在提升模型泛化性能。
  Overall:
  • 2000年，BMA(Bayesian Model Averaging)用于以模型为最优成员的概率的先验将各模型进行揉合从而降低风险。
  • 2017年，吴恩达就在<a href="https://mooc.study.163.com/course/2001281003?tid=2001391036&_trace_c_p_k2_=defef755139242b883937f7e9c67fcaf#/info">课上</a>讲EMA（指数移动平均）对模型的参数做平均，以求提高测试指标并增加模型鲁棒。
  • 2018年，有理论支撑，权值平滑能让测试误差落在更优平面。该思想更是能在训练中使用，成为SWA技术<a href="https://arxiv.org/pdf/1803.05407.pdf"><i class="fa fa-file-pdf-o"></i></a>。
  • 2019年，PyTorch开始在torchcontrib库支持swa。该技术也在Kaggle赛事中一展头角。
  • 2021年，NIPS一篇论文提出SWA Densly。
  • 2022年，谷歌提出“模型汤”，核心方法是Greedy soup:首先将所有模型按照验证集上准确度降序排列，然后逐个增加模型来进行权重平均，只有当得到的平均模型效果有提升时才考虑将当前的模型加入进来，这是一种简单的贪心策略<a href="https://arxiv.org/pdf/2203.05482.pdf"><i class="fa fa-file-pdf-o"></i></a>。
 </pre>

  <pre><b>区别</b>：SWA是取不同<font color="red">epoch</font>的模型，跨度更大。EMA是滑动取连续的<font color="red">iteration</font>的模型参数。</pre>
  <pre>EMA t时刻变量v的滑动平均值大致等于过去1 / ( 1 − β )个时刻v值的平均。即，
如果β = 0.99，则大致等于过去100个v值的平均。ema占内存少，不需要保存过去100个历史v值，就能够估计其均值。</pre>
  <pre>
对神经网络边的权重 weights 使用滑动平均，得到对应的影子变量shadow_weights。在训练过程仍然使用原来不带滑动平均的权重 weights，
以得到 weights 下一步更新的值，进而求下一步 weights 的影子变量 shadow_weights。之后在测试过程中使用shadow_weights 来代替
weights 作为神经网络边的权重，这样在测试数据上效果更好。</pre>
  <pre><font color="red">注：</font>使用EMA可能对BN层有帮助。减少BN层对batchsize的敏感。<a href="https://www.zhihu.com/question/269576836"><i class="fa fa-external-link"></i></a>
有好的预训练模型，可以将use_num_updates关掉，或者至少将其初始数值设大。不然会出现性能一开始骤降的情形。</pre>
  </ul>

  <h2>不平衡损失</h2>
  <ul>
  <li><b>思想溯源: </b> 从Topk到Focal loss</li>
  <pre>
  • 问题存在：①样本类别不平衡 ②样本难度不对等，简单样本支配训练，淹没（overwhelm training）评估损失的有效性。
  • ~2017年，
    1. 按照class比例加权重：最常用处理类别不平衡问题的方式
    2. <a href="https://www.zhihu.com/question/63581984">OHEM</a>：只保留loss最高的那些样本，完全忽略掉简单样本
    3. OHEM+按class比例sample：在前者基础上，再保证正负样本的比例
    3的结果比2要更差，其实这也表明，其实正负样本不平衡不是最核心的因素，而是由这个因素导出的<font color="red">easy example dominant</font>的问题。
  • 至今，Topk loss仍然被广泛用于kaggle比赛中。
  • 2017年，Loss max-pooling被提出，主要思想是：
    1. 通过pixel weighting functions自适应地对每个像素的contribution（实际展现的loss）进行re-weighting
    2. 通过普通的max-pooling在pixel-loss level上对pixel weighting function取最大
    3. 而这个最大值是传统loss（即每个像素损失的权重是相等的）的上界  
  • 同年，何恺明团队提出Focal loss。
    硬方案：OHEM 
    Focal loss可以看作soft OHEM。至于OHEM本身不能单独很好解决“简单样本支配”问题，私以为是focal loss可以"看全局"，而OHEM直接把easy ecample忽略使得许多pixel没有得到充分训练。
    软方案（sample reweighting）：Focal loss
    软性的难例加权，focal loss的思路是聚焦难例，并且自适应的削弱易例对最终的loss的贡献。
  </pre>
  </ul>


 <h2>Out防过拟合</h2>
  <ul>
    <li><b>思想溯源: </b> 从Dropout到Swapout</li>
    <pre>
      2013年，DropConnect [6]：只在连接处扔，神经元不扔。
      2014年，Dropout [1]：完全随机扔
      2015年，SpatialDropout [2]：按channel随机扔
      2016年，Stochastic Depth [3]：按res block随机扔
      2017年，Cutout [5]：在input层按spatial块随机扔
      2018年，DropBlock [4]：每个feature map上按spatial块随机扔
      由此，有的论文就是<a href="https://www.zhihu.com/question/300940578/answer/528785266"><font color="red">这么</font></a>来的。
    </pre>
  </ul>


  <h2>蒸馏学习、互学习</h2>
  <ul>
  <li></li>
  <li></li>

  </ul>

  <h2>各种归一化</h2>
  <ul>
      <li>GroupNorm</li>
      记住GN，就记得它的两个特例LN和IN了。
      <pre>
        将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值；这样与batchsize无关，不受其约束。
        事实上，GN的极端情况就是LN和IN，分别对应G等于1和G等于C，作者在论文中给出G设为32较好.
      </pre>
      <pre>
        当Conv bias遇上Norm:
        对于Norm之前的卷积是否使用偏置，也是有讲究的。
        MMDetection仓库就有友情提醒，当用户在使用Batch Norm或者Instance Norm时，MMCV的提示是非常贴心的。
        使用Layer Norm或者Group Norm（num_groups≠C）时，MMCV则不必抛出这个<a href="https://zhuanlan.zhihu.com/p/403444336">warning</a>。
      </pre>
      <pre>
        巧记
        HxW 看作一个实例 instance , 加一维 C 就看成 layter norm , 要是加一维 B 就是batch norm , 加部分C 就看做group
      </pre>
      
  </ul>


  <h2>各种激活函数</h2>
  <ul>
     <li>Swish(nn.SiLU)</li>
     <pre>
        没有充分的证据表明 某种激活函数 总是比 另一种激活函数 好。
        不过Swish的论文包含了一个有趣的讨论，关于什么激活函数是好的。
        作者指出，Swish工作得如此出色的原因是它的上无界，下有界，非单调，平滑。你可能已经注意到GELU也具有所有这些性质，我们稍后将讨论的最后一次激活函数也是这样。看来这就是激活研究的发展方向。
        <a href="https://zhuanlan.zhihu.com/p/409247652">激活函数的选择</a>
        <a href="https://zhuanlan.zhihu.com/p/352668984">介绍</a>
        <a href="https://krutikabapat.github.io/Swish-Vs-Mish-Latest-Activation-Functions/">Swish vs. Mish</a>
     </pre>
     
  </ul>


  
  <h2>优化器与<a href="https://atcold.github.io/pytorch-Deep-Learning/zh/week14/14-3/">正则化</a></h2>
  <ul>
    <pre>
      过拟合一般的解决方案：
      1.Early stop

      在模型训练过程中，提前终止。这里可以根据具体指标设置early stop的条件，比如可以是loss的大小，或者acc/f1等值的epoch之间的大小对比。

      2.More data

      用更多的数据集。增加样本也是一种解决方案，根据不同场景和数据有不同的数据增强方法。

      3.正则化

      常用的有L1、L2正则化

      4.Droup Out

      以一定的概率使某些神经元停止工作

      5.<a href="https://www.zhihu.com/question/275788133">BatchNorm</a>

      对神经元作归一化
    </pre>
    <pre>
      torch.optim集成了很多优化器，如SGD，Adadelta，Adam，Adagrad，RMSprop等，这些优化器中有一个参数weight_decay，用于指定权值衰减率，相当于L2正则化中的λ参数，
      注意torch.optim集成的优化器只有L2正则化方法，api中参数weight_decay 的解析是：weight_decay (float, optional): weight decay (L2 penalty) (default: 0)，
      这里可以看出其weight_decay就是正则化项的作用。可以如下设置L2正则化：
      optimizer = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.01)
      
      自定义正则化类：
      https://blog.csdn.net/zhang2010hao/article/details/89339327
    </pre>
    <a href="https://www.fast.ai/2018/07/02/adam-weight-decay/">Adam与weight decay</a>
  </ul>


  <h2>对抗训练</h2>
  <ul>
    <li><a href="https://wmathor.com/index.php/archives/1537/">NLP中的对抗训练</a></li>
    <li><a href="https://iphysresearch.github.io/blog/post/dl_notes/cs231n/cs231n_guest_lecture.adversarial_examples_and_adversarial_training/">对抗样本和对抗训练</a></li>
    <li>Min-Max公式</li>
    <p>Madry在2018年的ICLR论文<a href="https://arxiv.org/abs/1706.06083">Towards Deep Learning Models Resistant to Adversarial Attacks</a>中总结了之前的工作。总的来说，对抗训练可以统一写成如下格式：</p>
    <p>$$\min_{\theta}\mathbb{E}_{(x,y)\sim\mathcal{D}}\left[\max_{\Delta x\in\Omega}L(x+\Delta x, y;\theta)\right]$$</p>
    <p>其中$\mathcal{D}$代表数据集，$x$代表输入，$y$代表标签，$\theta$是模型参数，$L(x,y;\theta)$是单个样本的loss，$\Delta x$是扰动，$\Omega$是扰动空间。这个式子可以分步理解如下：</p>
    <ol>
      <li>往$x$里注入扰动$\Delta x$，$\Delta x$的目标是让$L(x+\Delta x, y;\theta)$越大越好，也就是说尽可能让现有模型的预测出错</li>
      <li>当然$\Delta x$也不是无约束的，它不能太大，否则达不到"看起来几乎一样"的效果，所以$\Delta x$要满足一定的约束，常规的约束是$||\Delta x||\leq \epsilon$，其中$\epsilon$是一个常数</li>
      <li>每个样本都构造出对抗样本$x+\Delta x$之后，用$(x+\Delta,y)$作为数据去最小化loss来更新参数$\theta$（梯度下降）</li>
      <li>反复交替执行1、2、3步</li>
    </ol>
  </ul>


  




  



<!-- 加载出评论，是使用Disqus的论坛短名（shortname）
A shortname is the unique identifier assigned to a Disqus site. 

https://segmentfault.com/a/1190000005773009
https://help.disqus.com/en/articles/1717111-what-s-a-shortname
https://blog.csdn.net/weixin_34327761/article/details/89630337
   -->

<div id="disqus_thread"></div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../comments/inlineDisqussions.js"></script>
<script src="../../js/disqus.js"></script>

                        </div>
                        <div class="col-md-4"></div>
                    </div>
                </div>
            </div>


            <div id="footer">
                <div class="container">
                    Built by <a href="https://github.com/oinkina">Oinkina</a> with
                    <a href="http://jaspervdj.be/hakyll">Hakyll</a>
                    using <a href="http://getbootstrap.com/">Bootstrap</a>,
                    <a href="http://www.mathjax.org/">MathJax</a>,
                    <a href="http://disqus.com/">Disqus</a>,
                    <a href="https://github.com/unconed/MathBox.js">MathBox.js</a>,
                    <a href="http://highlightjs.org/">Highlight.js</a>,
                    and <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>.
                </div>
            </div>
        </div>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

    <script src="../../bootstrap/js/bootstrap.min.js"></script>

    <script src="../../highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script src="../../js/footnotes.js"></script>

    <script src="../../comments/inlineDisqussions.js"></script>

    <noscript>Enable JavaScript for footnotes, Disqus comments, and other cool stuff.</noscript>

    </body>

</html>
