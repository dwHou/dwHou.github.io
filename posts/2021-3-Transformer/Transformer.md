## 带注解的Transformer

[TOC]

**by** <font color="brown">**harvardnlp**</font>

#### 前言

“Attention is All You Need”论文本身写得很清晰，但是传统的观点认为，正确实现它相当困难。所以这里一行行讲解、实现一个PyTorch版的Transformer。



ViT逐行解读

https://blog.csdn.net/qq_42418728/article/details/120365972?spm=1001.2014.3001.5502

自注意力

https://blog.csdn.net/qq_42418728/article/details/121723479
