<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>数据并行</title>

        <link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
        <link rel="stylesheet" href="../../fonts/Serif-Slanted/cmun-serif-slanted.css" />

        <!--BOOTSTRAP-->
        <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
        <!--mobile first-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <!--removed html from url but still is html-->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <!--font awesome-->
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

        <!--fonts: allan & cardo-->
        <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
        <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        <link href="../../css/sticky-footer-navbar.css" rel="stylesheet">

        <link href="../../css/default.css" rel="stylesheet">

        <link href="../../comments/inlineDisqussions.css" rel="stylesheet">

        <!-- SHJS0.5+ Syntax Highlighting in JavaScript-->
        <link type="text/css" rel="stylesheet" href="/plugins/shjs-0.6/css/sh_blacknblue.css">
        <link type="text/css" rel="stylesheet" href="/plugins/shjs-0.6/css/sh_laruence.css">
        <script type="text/javascrip" src="/plugins/shjs-0.6/sh_main.js"></script>
        <script type="text/javascript">
            $(function () {
                // 自动加载荧光
                sh_highlightDocument(/plugins/shjs-0.6/lang/‘, ‘.js’);
            });
        </script>


        <!--Highlight-->
        <link href="../../highlight/styles/github.css" rel="stylesheet">

        <link href="../../favicon.ico" rel="shortcut icon" />

        <!--<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
        <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <style>
        .post{width:170px;min-height:175px;padding-left:5px;padding-right:5px;float:left;border-left:1px solid #CCC;background-color:white;}
        div a:first-of-type .post { border-left: none; }
        .post:hover {filter: brightness(90%);}
        .post h3{margin:5px;font-size:75%;text-align:center}
        .post h4{margin:0px;font-size:50%;text-align:center}
        .post img{margin:0px;padding:2px;margin-bottom:10px;width:100%;height:155px}
        </style>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-49811703-1', 'dwHou.github.io');
          ga('require', 'linkid', 'linkid.js');
          ga('require', 'displayfeatures');
          ga('send', 'pageview');

        </script>

    </head>

    <body>
        <div id="wrap">
            <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
                <div class="container">
                    <!--Toggle header for mobile-->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand active" href="../../" style="font-size:20px;">De's blog</a>
                    </div>
                    <!--normal header-->
                    <div class="navbar-collapse collapse">
                        <ul class="nav navbar-nav navbar-right">
                            <li><a href="../../"><span class="glyphicon glyphicon-pencil"></span>  Blog</a></li>
                            <li><a href="../../about.html"><span class="glyphicon glyphicon-user"></span>  About</a></li>
                            <li><a href="../../contact.html"><span class="glyphicon glyphicon-envelope"></span>  Contact</a></li>
                            <li><a href="../../demo.html"><span class="glyphicon glyphicon-play"></span>  ALGO</a></li>
                        </ul>
                    </div><!--/.nav-collapse -->
                </div>
            </nav>


            <div id="content">
                <div class="container">
                    <div class="row">
                        <div class="col-md-8">
                            <h1>Distributed-DataParallel</h1>
                            <div style="font-size: 170%;">个人理解和总结</div>
                            <br>
                            <div class="info">
    <p style="font-family:CMSS; font-size:120%">Posted on Feb.19, 2020</p>

    <!--
        by dwHou
    -->
</div>
</br>


  <style>
    ul li {
      margin-top: 12px;
    }

    .tight-list li {
      margin-top: 6px;
    }
  </style>




  <h2>数据并行基础</h2>
  <a href="https://www.cnblogs.com/yh-blog/p/12877922.html">*</a>
  <ul>

      <li>模型并行</li>
      <pre>简易实现：略
      </pre>
      <li>数据并行</li>
      <pre>简易实现，为nn.DataParallel的底层(但后者稍有改进)，包括并行数据加载，每个GPU本地副本对一批数据正/反向传播，梯度发送到主进程，reduce归约操作计算平均梯度。然后将平均梯度结果发送回GPU，更新模型参数。

后端使用数据并行性和有效的网络通信软件库(例如NCCL)，可以实现使训练时间几乎线性减少。
      </pre>
      <li>nn.DataParallel</li>
      <pre>主GPU收集网络输出，计算损失函数值。损失值分散给各个GPU，每个GPU进行反向传播以计算梯度。最后，在主GPU上归约梯度、进行梯度下降，并更新主GPU上的模型参数。将更新后的模型参数复制到剩余的从属 GPU 中，以此来实现并行。

总而言之，只有正/反向传播在各个GPU上进行。其余的计算，包括更新权重均在主GPU进行。
      </pre>
      <pre><font color="red">注：</font>
这样会导致内存和GPU使用率出现很严重的<font color="green">负载不均衡现象</font>。因为在这里GPU0作为master来进行梯度的汇总和模型的更新，再将计算任务下发给其他GPU。

<font color="green">低效率</font>在于1.冗余数据副本, 2.在前向传播之前跨GPU进行模型复制, 3.主GPU不必要地收集所有的output, 4.在前向传播之前跨GPU进行模型复制, 5.梯度减少流水线机会未开发, 6.GPU利用率不均
      </pre>
  </ul>



<h2>分布式数据并行</h2>
<b>DDP</b> <a href="https://pytorch.org/tutorials/beginner/dist_overview.html">Overview, Author: Shen Li</a>
  <ul>

      <li>并行处理机制</li>
      <pre>分布式并行支持all-reduce, broadcast, send和receive等。
通过MPI实现CPU通信，NCCL实现GPU通信。可用于单/多机器多卡。

由于distributed相对于上述的nn.DataParallel是一个<font color="blue">底层</font>的API，所以我们要修改代码，使其能够独立运行在机器(节点)中。
      </pre>
      <img src="distributed.png">

      <pre>不同于DataParallel的单进程控制多GPU，在distributed的帮助下，我们只需编写一份代码，torch就会自动将其分配给n个进程。分别在n个GPU上运行，不再有主GPU，每个GPU执行相同的任务。
      </pre>

      <pre><font color="red">对比</font>DataParallel，DistributedDataParallel的优势如下：
1. 每个进程对应一个独立的训练过程，且只对梯度等少量数据进行信息交换。在各进程梯度计算完成之后，各进程需要将梯度进行汇总平均，然后再由 rank=0 的进程，将其 broadcast 到所有进程。之后，各进程用该梯度来独立的更新参数。而 DataParallel是梯度汇总到gpu0，反向传播更新参数，再广播参数给其他的gpu

2. 每个进程包含独立的解释器和 GIL。一般使用的Python解释器CPython：是用C语言实现Pyhon，是目前应用最广泛的解释器。全局锁使Python在多线程效能上表现不佳，全局解释器锁（Global Interpreter Lock）是Python用于同步线程的工具，使得任何时刻仅有一个线程在执行。
每个进程拥有独立的解释器和 GIL，消除了来自单个 Python 进程中的多个执行线程，模型副本或 GPU 的额外解释器开销和 GIL-thrashing ，因此可以减少解释器和GIL 使用冲突。这对于严重依赖 Python runtime 的 models 而言，比如说包含 RNN 层或大量小组件的models 而言，这尤为重要。
      </pre>

  </ul>

<h2>PyTorch并行和分布式训练</h2>
<ul>
    <li>介绍</li>
    torch.distributed的功能
    <pre>分布式并行训练(DDP)
Distributed Data-Parallel Training，能负责梯度的通信，来保障模型副本同步，并且通过并行计算加速训练。
PyTorch还支持基于RPC的分布式训练(RPC)，它更通用，集体通信(c10d)等特性，略。
    </pre>

    <li>数据并行</li>
    <pre>有多种选项。按照从简单到复杂，从原型到产品的思路，开发的路线将是：
1. 单卡训练
2. 单机多卡 DataParallel，如果想以最小的代码改动来加速训练
3. <a href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html">单机多卡</a> DistributedDataParallel，如果想更进一步加速
4. 多机多卡 DistributedDataParallel和<a href="https://github.com/pytorch/examples/blob/master/distributed/ddp/README.md">launching script</a>，如果想突破单机的限制
5. 弹性 torchelastic，训练中资源可以动态地加入和离开，可以容忍OOM错误.
（6. 数据并行可以和自带混合精度<a href="https://pytorch.org/docs/master/notes/amp_examples.html#">AMP</a>搭配使用）
    </pre>

    <li>nn.DataParallel</li>
    <pre>虽然很简易应用，它没法提供最好的性能。因为DataParallel在每次传播过程中都需要复制模型，并且单进程-多线程的并行会遭受GIL的冲突问题</pre>
    <li>nn.DistributedDataParallel</li>
    <pre>相比DataParallel，DistributedDataParallel需要多一步来设置，即初始化进程组(init_process_group).DDP使用多进程并行，所以在模型副本间没有GIL冲突，另外还运用了多种性能优化策略，可见论文<a href="https://arxiv.org/abs/2006.15704">VLDB'20</a>
DDP可以和模型并行很好结合，模型并行是在模型体积很大(如bert)时使用的。
    </pre>



    <li>开始分布式训练</li>
      <ul class = "level_2">
          <li><a href="https://pytorch.org/tutorials/beginner/dist_overview.html">概览</a></li>
          <li><a href="https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html">API文档</a></li>
          <li><a href="https://pytorch.org/docs/master/notes/ddp.html">笔记</a></li>
      </ul>

    <li>基础用例(8卡)</li>
    首先需要合理的初始化配置进程组。
    <pre class="sh_python">
import os
import sys
import tempfile
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim
import torch.multiprocessing as mp

from torch.nn.parallel import DistributedDataParallel as DDP


def setup(rank, world_size):
    if sys.platform == 'win32':
        # Distributed package only covers collective communications with Gloo
        # backend and FileStore on Windows platform. Set init_method parameter
        # in init_process_group to a local file.
        # Example init_method="file:///f:/libtmp/some_file"
        init_method="file:///{your local file path}"

        # initialize the process group
        dist.init_process_group(
            "gloo",
            init_method=init_method,
            rank=rank,
            world_size=world_size
        )
    else:
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'

        # initialize the process group
        dist.init_process_group("gloo", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()
    </pre>



</ul>



<!-- 加载出评论，是使用Disqus的论坛短名（shortname）
A shortname is the unique identifier assigned to a Disqus site. 

https://segmentfault.com/a/1190000005773009
https://help.disqus.com/en/articles/1717111-what-s-a-shortname
https://blog.csdn.net/weixin_34327761/article/details/89630337
   -->

<div id="disqus_thread"></div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../comments/inlineDisqussions.js"></script>
<script src="../../js/disqus.js"></script>

                        </div>
                        <div class="col-md-4"></div>
                    </div>
                </div>
            </div>


            <div id="footer">
                <div class="container">
                    Built by <a href="https://github.com/oinkina">Oinkina</a> with
                    <a href="http://jaspervdj.be/hakyll">Hakyll</a>
                    using <a href="http://getbootstrap.com/">Bootstrap</a>,
                    <a href="http://www.mathjax.org/">MathJax</a>,
                    <a href="http://disqus.com/">Disqus</a>,
                    <a href="https://github.com/unconed/MathBox.js">MathBox.js</a>,
                    <a href="http://highlightjs.org/">Highlight.js</a>,
                    and <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>.
                </div>
            </div>
        </div>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

    <script src="../../bootstrap/js/bootstrap.min.js"></script>

    <script src="../../highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script src="../../js/footnotes.js"></script>

    <script src="../../comments/inlineDisqussions.js"></script>

    <noscript>Enable JavaScript for footnotes, Disqus comments, and other cool stuff.</noscript>

    </body>

</html>
