

### ICLR 2018

#### :page_with_curl:Variational image compression with a scale hyperprior

本文思想：

本文描述了一种基于变分自动编码器（VAE）的图像压缩端到端可训练模型。 该模型结合了<font color="brown">超先验</font>以有效地捕获潜在表示（<font color="brown">latent representation</font>）中的空间依赖性。 这种超先验与辅助信息有关，这是一个几乎所有现代图像编解码器都通用的概念，但在使用人工神经网络 (ANN) 的图像压缩中很大程度上未被探索。

术语1：自编码器是一个输入和学习目标相同的神经网络，其结构分为编码器和解码器两部分。求解完成后，由编码器输出的隐含层特征h∈F，可视为输入数据X∈$\chi $的表征。

![img](https://bkimg.cdn.bcebos.com/formula/927a3d2569b30ef1cd9662eaa6a9d62a.svg)

按自编码器的不同，其编码特征可以是输入数据的压缩（收缩自编码器）、稀疏化（稀疏自编码器）或隐变量模型（变分自编码器）等。

> 我理解，总之h要么Frobenius范数（元素平方的和）变小，要么变稀疏，要么维度变小。

术语2：在贝叶斯统计中，超先验是超参数上的先验分布。

<font color="purple">背景方法：</font>

与所有有损压缩方法一样，ML-based图像压缩的工作原理很简单：对通常建模为像素强度向量 $x$ 的图像进行量化，减少存储或传输所需的信息量，但同时引入误差。 通常，直接量化的不是像素强度。 相反，找到图像的替代（潜在，<font color="brown">latent</font>）表示，即其他空间 $y$ 中的向量，并且在该表示中进行量化，产生离散值向量 $\hat{y}$。 因为它是离散的，所以可以使用熵编码方法（例如算术编码（Rissanen 和 Langdon，1981））对其进行无损压缩，以创建通过信道发送的比特流。 熵编码依赖于量化表示的先验概率模型，编码器和解码器都知道该模型（<font color="brown">熵模型</font>）。

在上面提到的一类基于 ANN 的图像压缩方法中，用于压缩潜在表示的熵模型通常表示为联合分布，甚至是完全分解的分布 $p_{\hat{y}}(\hat{y}）$。 请注意，我们需要区分潜在表示 $m(\hat{y})$ 的实际边缘分布和熵模型 $p_{\hat{y}}(\hat{y})$。 虽然熵模型通常被假定为具有某种参数形式，参数适应数据，但边缘是未知分布，由编码图像的分布和用于推断潜在表示$y$的方法引出。 编解码器可以实现的最小平均代码长度，使用$ p_{\hat{y}}$ 作为它们的共享熵模型，由两个分布之间的香农信息熵给出：

​													$$R = E_{\hat{y}∼m}[− log_{2} p_{\hat{y}}(\hat{y})]$$.

> 我理解，熵模型是量化表示的先验概率模型，可知；边缘分布由输入和编码器决定，是观测的频率，未知。

请注意，如果模型分布与边际分布相同，则此熵会最小化。 这意味着，例如，当潜在表示的实际分布中存在统计依赖性时，使用完全分解的熵模型$p_{\hat{y}}(\hat{y}）$将导致次优压缩性能。

<font color="purple">现有方法：</font>

传统方法：传统压缩方法提高其压缩性能的一种方法是传输辅助信息（side information）：从编码器发送到解码器的附加信息位，这些信息对熵模型做修改，旨在减少不匹配。 这是可行的，因为特定图像的边缘分布通常与设计压缩模型的图像集合的边缘分布有很大差异。在这个方案中，希望发送的辅助信息量平均小于通过将 $p_{\hat{y}}$ 更紧密地匹配到特定图像的边缘分布，而实现的码字的减少。

传统方法手工设计的辅助信息：例如，块划分结构。

例如，JPEG (1992) 块划分是固定的 8 × 8 像素大小。 然而，一些图像结构，例如大的均匀区域，可以通过一次考虑更大的块来更有效地表示。 出于这个原因， HEVC (2013) 将图像划分为可变大小的块，并且将分块结构作为辅助信息传递给解码器。 HEVC 解码器需要首先解码辅助信息，以便它可以使用正确的熵模型来解码块表示。 由于编码器可以自由选择优化熵模型的块划分，因此该方案可用于实现更有效的压缩。

> 所以JPEG是完全分解的熵模型$p_{\hat{y}}(\hat{y}）$，而HEVC有块划分结构作为辅助信息，利用了统计依赖性。

端到端可训练模型的辅助信息：本文提出的一种体系是，熵模型参数的先验可以视为辅助信息，即latent的超先验。

<font color="purple">本文方法：</font>

2017年，本文作者Ballé 等人提出的模型，有一个完全分解的先验。

2018年的本文在它的基础上扩展了一个超先验——潜在表示的空间相邻元素往往在尺度上一起变化。

传统方法：

<font color="brown">$x$ → 参数分析变换（parametric analysis transform）$g_{a}$ → 算术编码器$AE$ → 传输压缩信号 → 算术解码器$AD$ → 参数合成变换（parametricsynthesistransform）$g_{s}$ → $\hat{x}$</font>

智能方法：

将变换 $g_{a}$ 和 $g_{s}$ 视为通用参数化函数，例如人工神经网络 (ANN)，而不是传统压缩方法中的线性变换。然后参数 $\theta_{g}$ 和 $\phi_{g}$ 封装了神经元的权重等。

写成交叉熵的形式就是：

​										$$R = E_{x∼p_{x}}[− log_{2} p_{\hat{y}}(Q(g_{a}(x,\phi _{g})))]$$.		

其中 $Q $代表量化函数，$p_{\hat{y}}$ 是熵模型。 在这种情况下，latent的边缘分布来自（未知的）图像分布 $p_{x}$ 和分析变换$g_{a}$的属性。

量化的粗糙度（步长），可以形成码率和失真的trade-off。可以将各种压缩方法视为最小化这两个量的加权和。 形式上，我们可以通过$ λ $参数化问题，$λ$ 是失真项的权重：$L=λ∗D+R$，不同的应用需要不同的权衡，因此需要不同的 $λ $值。									

为了能够使用<font color="brown">梯度下降法</font>来优化模型在变换参数（$\theta_{g}$ 和 $\phi_{g}$ ）上的性能，需要放宽问题，因为<font color="brown">由于量化，关于 $\phi_{g}$的梯度几乎处处为零。</font>

已研究的近似值包括替换量化器的梯度（Theis 等人，2017 ），以及在训练期间用加性均匀噪声替换量化器本身（Ballé 等人，2016b）。 

在这里，我们采用后一种方法，<font color="brown">训练时用加性均匀噪声，在将模型用作压缩方法时切换回实际量化。 </font>我们用$\tilde{y}$ 表示“加噪声”表示，而$\hat{y}$表示量化表示。

<img src="/Users/DevonnHou/Library/Application Support/typora-user-images/image-20221122174851217.png" alt="image-20221122174851217" style="zoom:50%;" />

左图：将变换编码模型表示为生成贝叶斯模型，以及相应的变分推理模型。 节点代表随机变量或参数，箭头表示它们之间的条件依赖。 右图：显示压缩模型的操作结构的图表。 箭头表示数据流，方框表示数据的转换。 标有 U|Q的框表示在训练期间添加均匀噪声（生成$\tilde{y}$），或在测试期间进行量化和算术编码/解码（生成$\hat{y}$）。



术语3：

信息量，或者叫<font color="brown">自信息</font>（self-information）,其代表一个事件所能够提供信息的多少，具体计算方式为: $I(x)=−log⁡P(x)$ 。其是基于这样的想法进行信息量化的，**一个不太可能发生的事件发生了，要比一个非常可能发生的事件提供更多的信息**(概率小，log值高)。

但是，自信息只能衡量单个事件的信息量，而整个系统呈现的是一个分布因此在信息论中，使用<font color="brown">信息熵</font>来对概率分布进行量化，即$H(X)=E_{X∼P}[I(x)]=−E_{X∼P}[P(x)]$，

具体也就是$$ {\textstyle \sum_{k=1}^{N}} p_{k}log_{2}{\frac{1}{p_{k}}}$$

**信息熵**代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。

但以上是建立在知道真实概率分布的情况下的最优策略，但实际很多情况我们的策略并不是最优。所以引入<font color="brown">交叉熵</font>，用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。

$$ {\textstyle \sum_{k=1}^{N}} p_{k}log_{2}{\frac{1}{q_{k}}}$$，这里$p_{k}$表示真实分布，$q_{k}$表示非真实分布

最低（最理想）的交叉熵 = 信息熵，这也是为什么在机器学习中的算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布。

我们如何去衡量不同策略之间的差异呢？这就需要用到<font color="brown">相对熵</font>，其用来衡量两个取值为正的函数或概率分布之间的差异，即：

$$KL(f(x)||g(x))={\textstyle \sum_{x \in X}} f(x)log_{2}{\frac{f(x)}{g(x)}}$$

某个策略和最优策略之间的差异：

**相对熵 = 某个策略的交叉熵 - 信息熵**，$$ {\textstyle \sum_{k=1}^{N}} p_{k}log_{2}{\frac{1}{q_{k}}} - {\textstyle \sum_{k=1}^{N}} p_{k}log_{2}{\frac{1}{p_{k}}} = {\textstyle \sum_{k=1}^{N}} p_{k}log_{2}{\frac{p_{k}}{q_{k}}}$$

编解码用到的是，熵的本质的另一种解释：**最短平均编码长度**

> 编码方案完美时，最短平均编码长度的是多少





### CVPR 2020

#### :page_with_curl:Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules

现有方法分析：

上下文自适应熵模型，

### CVPR 2022

#### :page_with_curl:ELIC: Efficient Learned Image Compression

### 5th CLIC

#### :page_with_curl: PO-ELIC: Perception-Oriented Efficient Learned Image Coding

在作者此前发表的SOTA方法ELIC的基础上，加上了基于感知的损失函数。









