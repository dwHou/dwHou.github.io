<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>流匹配 和 扩散模型</title>

    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMath()"></script>

    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Source+Code+Pro:wght@400;500;600&family=Noto+Sans+SC:wght@300;400;500;700&display=swap');

        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --bg-tertiary: #e9ecef;
            --primary: #0066cc;
            --secondary: #00a896;
            --accent: #ff6b6b;
            --text-primary: #2c3e50;
            --text-secondary: #6c757d;
            --border-color: #dee2e6;
            --shadow-light: rgba(0, 0, 0, 0.05);
            --shadow-medium: rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Noto Sans SC', 'Inter', sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            overflow: hidden;
            height: 100vh;
            width: 100vw;
        }

        /* Presentation container */
        .presentation {
            position: relative;
            width: 100%;
            height: 100%;
            z-index: 1;
        }

        /* Individual slides */
        .slide {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            padding: 60px 80px;
            display: flex;
            flex-direction: column;
            justify-content: flex-start;
            opacity: 0;
            visibility: hidden;
            transform: translateX(50px);
            transition: all 0.4s ease;
            overflow-y: auto;
            background: var(--bg-primary);
        }

        .slide.active {
            opacity: 1;
            visibility: visible;
            transform: translateX(0);
        }

        .slide.exit {
            opacity: 0;
            transform: translateX(-50px);
        }

        /* Typography */
        h1 {
            font-family: 'Inter', 'Noto Sans SC', sans-serif;
            font-size: 3.5rem;
            font-weight: 700;
            color: var(--primary);
            margin-bottom: 24px;
            letter-spacing: -0.5px;
        }

        h2 {
            font-family: 'Inter', 'Noto Sans SC', sans-serif;
            font-size: 2.5rem;
            font-weight: 600;
            color: var(--primary);
            margin-bottom: 32px;
            padding-bottom: 16px;
            border-bottom: 3px solid var(--secondary);
        }

        h3 {
            font-size: 1.75rem;
            color: var(--secondary);
            margin: 28px 0 16px 0;
            font-weight: 600;
        }

        h4 {
            font-size: 1.4rem;
            color: var(--text-primary);
            margin: 24px 0 12px 0;
            font-weight: 600;
        }

        h5, h6 {
            font-size: 1.2rem;
            color: var(--text-primary);
            margin: 20px 0 10px 0;
            font-weight: 600;
        }

        p, li {
            font-size: 1.3rem;
            line-height: 1.8;
            color: var(--text-primary);
            margin-bottom: 12px;
        }

        /* Code blocks */
        pre {
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin: 16px 0;
            font-size: 1.1rem;
            overflow-x: auto;
            font-family: 'Source Code Pro', monospace;
            box-shadow: 0 2px 8px var(--shadow-light);
        }

        code {
            font-family: 'Source Code Pro', monospace;
            background: var(--bg-tertiary);
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.9em;
            color: var(--accent);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 1em;
            color: var(--text-primary);
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
            font-size: 1.1rem;
            box-shadow: 0 2px 8px var(--shadow-light);
        }

        th {
            background: var(--primary);
            color: white;
            padding: 16px;
            text-align: left;
            font-weight: 600;
        }

        td {
            padding: 14px 16px;
            border: 1px solid var(--border-color);
            background: var(--bg-primary);
        }

        tr:hover td {
            background: var(--bg-secondary);
        }

        /* Lists */
        ul, ol {
            margin: 16px 0 16px 32px;
        }

        li {
            margin: 10px 0;
        }

        li::marker {
            color: var(--secondary);
        }

        /* Links */
        a {
            color: var(--primary);
            text-decoration: none;
            border-bottom: 2px solid transparent;
            transition: all 0.3s ease;
        }

        a:hover {
            color: var(--secondary);
            border-bottom-color: var(--secondary);
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 12px var(--shadow-medium);
            margin: 16px 0;
        }

        /* Blockquotes */
        blockquote {
            background: var(--bg-secondary);
            border-left: 4px solid var(--secondary);
            padding: 16px 20px;
            margin: 16px 0;
            font-style: italic;
            color: var(--text-secondary);
        }

        /* Blockquote headings - prevent oversized fonts */
        blockquote h1 { font-size: 1.2rem !important; font-weight: 600; margin: 0.5em 0; }
        blockquote h2 { font-size: 1.1rem !important; font-weight: 600; margin: 0.5em 0; }
        blockquote h3 { font-size: 1rem !important; font-weight: 600; margin: 0.5em 0; }
        blockquote h4,
        blockquote h5,
        blockquote h6 {
            font-size: 0.95rem !important;
            font-weight: 600;
            margin: 0.5em 0;
        }

        /* Blockquote heading colors */
        blockquote h1,
        blockquote h2,
        blockquote h3,
        blockquote h4,
        blockquote h5,
        blockquote h6 {
            color: var(--secondary);
            font-style: normal;
        }

        /* Horizontal rule */
        hr {
            border: none;
            border-top: 2px solid var(--border-color);
            margin: 32px 0;
        }

        /* Progress bar */
        .progress-container {
            position: fixed;
            bottom: 0;
            left: 0;
            width: 100%;
            height: 4px;
            background: var(--bg-tertiary);
            z-index: 100;
        }

        .progress-bar {
            height: 100%;
            background: linear-gradient(90deg, var(--primary), var(--secondary));
            transition: width 0.3s ease;
        }

        /* Footer - centered bottom, semi-transparent */
        .footer-text {
            position: fixed;
            bottom: 10px;
            left: 0;
            right: 0;
            text-align: center;
            font-size: 0.85rem;
            color: rgba(108, 117, 125, 0.5);
            opacity: 0.5;
            z-index: 50;
            pointer-events: none;
        }

        /* Page number - right bottom corner */
        .page-number {
            position: fixed;
            bottom: 15px;
            right: 20px;
            font-size: 0.9rem;
            color: var(--text-secondary);
            background: rgba(255, 255, 255, 0.5);
            padding: 5px 12px;
            border-radius: 15px;
            border: 1px solid var(--border-color);
            z-index: 50;
            pointer-events: none;
            box-shadow: 0 2px 8px var(--shadow-light);
        }

        /* Navigation hints */
        .nav-hint {
            position: fixed;
            bottom: 20px;
            left: 30px;
            font-size: 1rem;
            color: var(--text-secondary);
            z-index: 100;
        }

        .nav-hint kbd {
            background: var(--bg-tertiary);
            border: 1px solid var(--border-color);
            border-radius: 4px;
            padding: 4px 10px;
            margin: 0 4px;
            font-family: 'Inter', sans-serif;
        }

        /* Title slide special */
        .slide.title-slide {
            justify-content: center;
            align-items: center;
            text-align: center;
        }

        .slide.title-slide h1 {
            font-size: 5rem;
            margin-bottom: 32px;
            color: var(--primary);
        }

        /* Fullscreen button */
        .fullscreen-btn {
            position: fixed;
            top: 20px;
            right: 30px;
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            color: var(--text-primary);
            padding: 10px 16px;
            border-radius: 6px;
            cursor: pointer;
            font-family: 'Noto Sans SC', 'Inter', sans-serif;
            font-size: 1rem;
            z-index: 100;
            transition: all 0.3s ease;
            box-shadow: 0 2px 8px var(--shadow-light);
        }

        .fullscreen-btn:hover {
            background: var(--primary);
            color: white;
            border-color: var(--primary);
        }

        /* Single-tier chapter navigation (flattened) */
        .chapter-nav {
            position: fixed;
            bottom: 50px;
            left: 0;
            right: 0;
            text-align: center;
            z-index: 100;
            padding: 10px 20px;
            overflow-x: auto;
            white-space: nowrap;
        }

        .chapter-nav::-webkit-scrollbar {
            height: 4px;
        }

        .chapter-nav::-webkit-scrollbar-thumb {
            background: rgba(0, 102, 204, 0.3);
            border-radius: 2px;
        }

        .chapter-nav .chapter {
            cursor: pointer;
            padding: 6px 12px;
            font-size: 0.9rem;
            font-weight: 400;
            color: #888;
            transition: all 0.3s ease;
            border-radius: 4px;
            white-space: nowrap;
            display: inline-block;
        }

        .chapter-nav .chapter:hover {
            color: var(--primary);
            background: var(--bg-secondary);
        }

        .chapter-nav .chapter.active {
            color: #0066cc;
            font-weight: 600;
            background: var(--bg-secondary);
            border-bottom: 2px solid #0066cc;
        }

        .chapter-nav .chapter[data-level="1"] {
            font-weight: 500;
        }

        .chapter-nav .chapter[data-level="2"] {
            font-size: 0.85rem;
        }

        .chapter-nav .chapter[data-level="3"] {
            font-size: 0.8rem;
            font-style: italic;
        }

        .chapter-nav .separator {
            color: #444;
            padding: 0 8px;
            display: inline-block;
        }

        /* TOC Icon */
        .toc-icon {
            position: fixed;
            top: 20px;
            left: 20px;
            width: 44px;
            height: 44px;
            background: white;
            border: 2px solid var(--border-color);
            border-radius: 8px;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            z-index: 200;
            transition: all 0.3s ease;
            color: #888;
            box-shadow: 0 2px 8px var(--shadow-light);
        }

        .toc-icon:hover {
            background: var(--bg-secondary);
            color: var(--primary);
            box-shadow: 0 4px 12px var(--shadow-medium);
            transform: scale(1.05);
        }

        /* TOC Panel */
        .toc-panel {
            position: fixed;
            top: 20px;
            left: 20px;
            width: 320px;
            max-height: 80vh;
            background: white;
            border: 2px solid var(--border-color);
            border-radius: 12px;
            box-shadow: 0 8px 32px var(--shadow-medium);
            z-index: 200;
            display: none;
            flex-direction: column;
            overflow: hidden;
        }

        .toc-panel.active {
            display: flex;
            animation: slideIn 0.3s ease;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateX(-20px);
            }
            to {
                opacity: 1;
                transform: translateX(0);
            }
        }

        .toc-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 16px 20px;
            border-bottom: 2px solid var(--border-color);
            background: var(--bg-secondary);
        }

        .toc-header h3 {
            margin: 0;
            font-size: 1.1rem;
            color: var(--primary);
            font-weight: 600;
        }

        .toc-close {
            background: none;
            border: none;
            color: #888;
            font-size: 1.8rem;
            line-height: 1;
            cursor: pointer;
            padding: 0;
            width: 30px;
            height: 30px;
            display: flex;
            align-items: center;
            justify-content: center;
            border-radius: 4px;
            transition: all 0.2s;
        }

        .toc-close:hover {
            background: rgba(0, 0, 0, 0.05);
            color: #000;
        }

        .toc-content {
            flex: 1;
            overflow-y: auto;
            padding: 12px 0;
        }

        .toc-content::-webkit-scrollbar {
            width: 6px;
        }

        .toc-content::-webkit-scrollbar-thumb {
            background: rgba(0, 102, 204, 0.3);
            border-radius: 3px;
        }

        .toc-item {
            display: flex;
            align-items: baseline;
            padding: 10px 20px;
            cursor: pointer;
            transition: all 0.2s;
            color: var(--text-secondary);
            border-left: 3px solid transparent;
        }

        .toc-item:hover {
            background: var(--bg-secondary);
            border-left-color: var(--primary);
            color: var(--text-primary);
        }

        .toc-item.active {
            background: var(--bg-secondary);
            border-left-color: var(--primary);
            color: var(--primary);
            font-weight: 500;
        }

        .toc-item[data-level="1"] {
            padding-left: 20px;
            font-weight: 500;
            font-size: 0.95rem;
        }

        .toc-item[data-level="2"] {
            padding-left: 40px;
            font-size: 0.85rem;
        }

        .toc-item[data-level="3"] {
            padding-left: 60px;
            font-size: 0.8rem;
            font-style: italic;
        }

        .toc-item[data-level="4"] {
            padding-left: 80px;
            font-size: 0.75rem;
            color: var(--text-muted);
        }

        .toc-item[data-level="5"] {
            padding-left: 100px;
            font-size: 0.7rem;
            color: var(--text-muted);
            opacity: 0.8;
            font-style: italic;
        }

        .toc-title {
            flex: 1;
            overflow: hidden;
            text-overflow: ellipsis;
            white-space: nowrap;
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .slide { padding: 48px; }
            h1 { font-size: 2.8rem; }
            h2 { font-size: 2rem; }

            .chapter-nav {
                bottom: 45px;
                padding: 8px 16px;
            }

            .chapter-nav .chapter {
                font-size: 0.85rem;
                padding: 5px 10px;
            }

            .toc-panel {
                width: 280px;
            }
        }

        @media (max-width: 768px) {
            .slide { padding: 36px 24px; }
            h1 { font-size: 2.2rem; }
            h2 { font-size: 1.6rem; }
            table { font-size: 1rem; }
            th, td { padding: 10px; }

            .chapter-nav {
                bottom: 40px;
                padding: 6px 12px;
            }

            .chapter-nav .chapter {
                font-size: 0.75rem;
                padding: 4px 8px;
            }

            .footer-text {
                font-size: 0.7rem;
            }

            .page-number {
                font-size: 0.8rem;
                bottom: 10px;
                right: 10px;
            }

            .toc-panel {
                width: calc(100vw - 40px);
                left: 20px;
                right: 20px;
            }

            .toc-icon {
                width: 40px;
                height: 40px;
            }
        }
    </style>
</head>
<body>


    <button class="fullscreen-btn" onclick="toggleFullscreen()">[ F11 全屏 ]</button>

    <div class="presentation">
        <!-- Slide 1 -->
        <div class="slide active" data-slide="1" data-chapter="流匹配 和 扩散模型">
<h1 id="_1">流匹配 和 扩散模型</h1>
<p><a href="https://youtu.be/GCoP2w-Cqtg?si=uDdnmfEIZhUb1jxc">MIT 6.S184</a></p>
<p>这门课：Flow/Diffusion模型的理论与实践</p>
<ul>
<li>理论：第一性原理，必要而最少量的数学知识 ODE、SDE</li>
<li>实践：如何实现</li>
</ul>
        </div>

        <!-- Slide 2 -->
        <div class="slide" data-slide="2" data-chapter="第零章 学习资源">
<h2 id="_1">第零章 学习资源</h2>
<p>https://diffusion.csail.mit.edu/</p>
<p>网站包括幻灯片，以及三个实验，以及<a href="https://arxiv.org/pdf/2506.02070">课堂笔记</a>。</p>
        </div>

        <!-- Slide 3 -->
        <div class="slide" data-slide="3" data-chapter="第一章 利用随机微分方程的Gen AI">
<h2 id="gen-ai">第一章 利用随机微分方程的Gen AI</h2>
<h3 id="_1">第一节：从生成到采样</h3>
<p>我们将图像/视频/蛋白质表示为向量</p>
$z_{图像}\in R^{H \times R \times 3} $
<p>，，（N个原子有3坐标）。</p>
<p>一张图像的“好”程度 ≈ 它在数据分布下的可能性有多高</p>
<blockquote>
<p>学术一点的说法：图像的质量可以近似等同于它在数据分布中的似然性</p>
</blockquote>
<p>生成就是从数据分布中采样</p>
<p>数据分布一般用概率密度函数$p_{data}$表示。</p>
<p>数据集包含了数据分布中有限个数的样本：$z_1,...,z_N \sim p_{data}$</p>
<p>条件生成指的是从条件分布中采样：$z \sim p_{data}(\cdot \mid y)$，比如 $y$ 是提示词。意味着给定这个提示，数据的分布是什么。这是我们最感兴趣的课题。</p>
<p>生成模型将初始分布（例如高斯分布）中的样本转换为数据分布中的样本。</p>
$x \sim p_{init}$
<p>➡️    ➡️  </p>
<h3 id="_2">第二节 流模型与扩散模型</h3>
$z_{视频}\in R^{T \times H \times R \times 3} $$z_{分子结构}\in R^{N \times 3} $$Generative Model$$z \sim p_{data}$
        </div>

        <!-- Slide 4 -->
        <div class="slide" data-slide="4" data-chapter="第一章 利用随机微分方程的Gen AI">
<h4 id="21">2.1 流模型</h4>
<h6 id="211">2.1.1 基本术语和概念</h6>
<p>流的基本对象：轨迹（Trajectory）、向量场（Vector Field）、常微分方程（ODE）</p>
<p><font color="blue">1. 轨迹：$X: [0, 1] \to \mathbb{R}^d, \quad t \mapsto X_t$ </font></p>
<blockquote>
<p>[!NOTE]</p>
<p>$\to$ 表示函数的定义域和值域之间的映射关系；</p>
<p>$\mapsto$ 表示具体给定一个元素映射到另一个元素；</p>
<p>轨迹终点 $X_1$ 落在真实数据分布上。</p>
</blockquote>
<p><font color="blue">2. 向量场：$u: \mathbb{R}^d \times [0,1] \to \mathbb{R}^d, \quad (x, t) \mapsto u_t(x)$</font></p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>笛卡尔积</strong>在这里的作用是“构造一个联合空间”，就像编程里写<code>def f(x: Vector, t: float)-&gt; Vector:</code>，数学中我们不能写两个参数名，所以就把它们<strong>打包成一个二元组 $(x, t)$</strong>，而这个“打包”的空间就是 $\mathbb{R}^d \times [0,1]$。</p>
</blockquote>
<p><font color="blue">3. 常微分方程</font>：描述轨迹上的条件</p>
<p><font color="blue">$X_0 = x_0 \;(初始条件)$</font> 沿着向量场指定的方向前进。</p>
<p><font color="blue">$\frac{d}{dt}X_t = u_t(X_t)$ （ODE）</font></p>
        </div>

        <!-- Slide 5 -->
        <div class="slide" data-slide="5" data-chapter="第一章 利用随机微分方程的Gen AI">
<p>轨迹的导数或速度 是由 $X_t$ 当前所在位置的向量 $u_t(X_t)$ 给出的。</p>
<blockquote>
<p>[!TIP]</p>
<p>也许我们中的一些人听说过ODE在工程和物理学中是力学的基础。但“<font color="brown">流</font>”这个术语不太常见。流是遵循ODE的轨迹的集合。</p>
<p><font color="brown">本质上是我们收集大量针对不同初始条件的解决方案，然后将它们全部收集到一个函数中，并称之为流。</font></p>
</blockquote>
<p><font color="blue">4. 流：$\psi: \mathbb{R}^d \times [0,1] \to \mathbb{R}^d, \quad (x_0, t) \mapsto \psi_t(x_0)$</font></p>
<blockquote>
<p><strong>流</strong> $\psi$是所有不同起点 $x_0$ 的轨迹集合，即整个系统的“流动结构”。</p>
</blockquote>
<p>我们希望对于每个初始条件$x_0$，轨迹$\psi_t(x_0)$都是下面这个ODE的解：</p>
$\psi_0(x_0) = x_0$

$\frac{d}{dt}\psi_t(x_0) = u_t(\psi_t(x_0))$

<p>所以：</p>
<ul>
<li><font color="blue">ODE由向量场（VF）定义。</font></li>
<li><font color="blue">轨迹是ODE的解。</font></li>
<li><font color="blue">流则是各种初始条件的轨迹的集合。</font></li>
</ul>
        </div>

        <!-- Slide 6 -->
        <div class="slide" data-slide="6" data-chapter="第一章 利用随机微分方程的Gen AI">
<p><img src="assets/image-20250714163807435.png" alt="image-20250714163807435" style="zoom:30%;" /></p>
<blockquote>
<p>图示红色网格是轨迹，蓝色箭头是向量场。</p>
</blockquote>
<h6 id="212">2.1.2 定理</h6>
<p>ODEs 解的存在性与唯一性定理</p>
<p><strong>定理（皮卡–林德勒夫定理）</strong>：<br />
如果向量场 $u_t(x)$ 是连续可微的，且其导数有界，那么下面这个常微分方程（ODE）：$X_0 = x_0\:, \quad \frac{d}{dt}X_t = u_t(X_t)$ </p>
<p>存在唯一解。换句话说，流映射是存在的。更一般地说，只要向量场是 <strong>Lipschitz 连续的</strong>，结论仍然成立。</p>
<blockquote>
<p><strong>Lipschitz 连续</strong>是一种比连续更强，比可微略弱的函数光滑性条件，在分析和微分方程中非常重要。</p>
<p>[!TIP]</p>
<p>在机器学习实际应用中，常微分方程（ODE）或流（flow）的问题通常都存在<strong>唯一解</strong>。</p>
<p>你上过的大多数课程中，这已经被隐式假设了。</p>
</blockquote>
<h6 id="213-ode">2.1.3 示例：线性ODE</h6>
<p><strong><font color="brown">Flow-based 模型是在学习一个确定性的（deterministic）向量场，间接决定轨迹。轨迹由向量场通过常微分方程生成。</font></strong></p>
<p>简单的线性向量场：$u_t(x) = -\theta x \; (\theta > 0)$</p>
<p>断言：流由下式给出：$\Psi_t(x_0) = e^{-\theta t} x_0$</p>
        </div>

        <!-- Slide 7 -->
        <div class="slide" data-slide="7" data-chapter="第一章 利用随机微分方程的Gen AI">
<blockquote>
<ul>
<li>断言（Claim）在数学中表示一个待证明的断言、结论或命题。</li>
<li>$\psi$ 发音为 /saɪ/ 或 /psaɪ/</li>
</ul>
</blockquote>
<p>证明：</p>
<ol>
<li>初始条件：</li>
</ol>
$\Psi_t(x_0) = e^{0} x_0 = x_0$

<ol>
<li>ODE：</li>
</ol>
$\frac{d}{dt}\Psi_t(x_0) = \frac{d}{dt}(e^{-\theta t} x_0) = -\theta e^{-\theta t}x_0 = -\theta \psi_t(x_0) = u_t(\psi_t(x_0))$

<p>不同初始条件的轨迹：</p>
$y$
<p>轴表示初始条件。轨迹呈指数级趋近于零。</p>
<p><img src="assets/image-20250713225445411.png" alt="image-20250713225445411" style="zoom:50%;" /></p>
<h6 id="214-ode">2.1.4 ODE数值模拟——欧拉法</h6>
<p>不幸的是，在大多数情况，这并不容易，你不能只是手动找到ODE的解。</p>
<p>我们需要做的是模拟它。</p>
<p><a name="algorithm1">算法1</a>：欧拉法模拟ODE</p>
<p>输入：向量场$u_t$，初始条件$x_0$，步数$n$</p>
<ol>
<li>
<p>设 $t = 0$</p>
</li>
<li>
<p>设步长 $h = \frac{1}{n}$</p>
</li>
</ol>
        </div>

        <!-- Slide 8 -->
        <div class="slide" data-slide="8" data-chapter="第一章 利用随机微分方程的Gen AI">
<ol>
<li>
<p>设 $X_0 = x_0$</p>
</li>
<li>
<p>对 $i = 1, \cdots, n-1$ 循环：</p>
</li>
</ol>
<p>​    $X_{t+h} = X_t + hu_t(X_t)$</p>
<p>​    更新$t$为$t+h$</p>
<ol>
<li>
<p>结束循环</p>
</li>
<li>
<p>返回轨迹：$X_0, X_h, X_{2h}, \cdots, X_1$</p>
</li>
</ol>
<blockquote>
<p>[!TIP]</p>
<p>这是sampling的算法（生成的过程），比较简单。困难的是$u_t^\theta$训练的部分。</p>
</blockquote>
<h6 id="215">2.1.5 生成模型</h6>
<ol>
<li>
<p>流模型：$p_{init} \xrightarrow{\text{ODE}} p_{data}$</p>
</li>
<li>
<p>神经网络：将向量场变成一个神经网络。</p>
</li>
</ol>
<p>​                $u_t^\theta : \mathbb{R}^d \times [0, 1] \to \mathbb{R}^d$，$\theta$ 是网络参数</p>
<ol>
<li>随机初始条件：由于ODE是确定性的，所以还不能生成整个分布。但我们可以使初始条件随机化。</li>
</ol>
$X_0 \sim p_{init}$

<ol>
<li>常微分方程：</li>
</ol>
$\frac{d}{dt}X_t = u_t^\theta(X_t)$
        </div>

        <!-- Slide 9 -->
        <div class="slide" data-slide="9" data-chapter="第一章 利用随机微分方程的Gen AI">
<ol>
<li>目标：</li>
</ol>
$X_1 \sim p_{data}$

<p><img src="assets/image-20250714163036782-2481846.png" alt="image-20250714163036782" style="zoom:30%;" /></p>
<p><img src="assets/image-20250714163319333.png" alt="image-20250714163319333" style="zoom:30%;" /></p>
<p>后面我们会学到，这幅图描述就是用高斯概率路径的边际向量场进行基于欧拉法的ODE数值模拟。</p>
<p><img src="assets/image-20250714163356536.png" alt="image-20250714163356536" style="zoom:30%;" /></p>
<h4 id="22">2.2 扩散模型</h4>
<p>扩散模型本质上扩展了我们刚才讨论过的想法，但采用随机微分方程。</p>
<h6 id="221">2.2.1 基本术语和概念</h6>
<p>扩散模型的基本对象：随机过程（Stochastic process）、向量场（Vector Field）、常微分方程（ODE）</p>
<p><font color="blue">1. 随机过程：扩散模型的解是随机的轨迹，也称为随机过程。</font></p>
$X_t, \; (0 \le t \le 1)$
<p>是随机变量</p>
$X: [0, 1] \to \mathbb{R}^d, \quad t \mapsto X_t$
<p>，但此时可以从中抽取样本，所以本身是随机的。这些轨迹的集合更像是它们发生的可能性。</p>
<p><font color="blue">2. 向量场：$u: \mathbb{R}^d \times [0,1] \to \mathbb{R}^d, \quad (x, t) \mapsto u_t(x)$</font></p>
<p><font color="blue">3.  扩散系数：$\sigma: [0,1] \to \mathbb{R}_{\ge 0}, \quad t \mapsto \sigma_t$</font>，由它向ODE注入随机性。</p>
$X$$X$
        </div>

        <!-- Slide 10 -->
        <div class="slide" data-slide="10" data-chapter="第一章 利用随机微分方程的Gen AI">
<p><font color="blue">3. 随机微分方程：</font></p>
<p><font color="blue">$X_0 = x_0 \;(初始条件)$</font> </p>
<p><font color="blue">$dX_t = \underbrace{u_t(X_t)dt}_{\text{ODE}} + \underbrace{\sigma_tdW_t}_{stochastic/noise}$ （SDE）</font></p>
<p>表示： $X_t$ 随时间演化，它的变化由两个部分组成：</p>
<ol>
<li><strong>确定性部分</strong>：它会朝着一个“向量场”或“趋势” $u_t(X_t)$ 的方向走（ODE）。</li>
<li><strong>随机部分</strong>：它还会叠加一些“不可预测的扰动”——这些扰动由布朗运动 $dW_t$ 产生，并通过一个缩放因子 $\sigma_t$ 控制其强度。</li>
</ol>
$W_t$
<p>表示布朗运动，在数学中通常被建模为一个 Wiener 过程（维纳过程）。</p>
<p><font color="blue">4. 布朗运动：</font></p>
<p>随机过程：$W： (W_t)_{t \ge 0}$，$W_t \in \mathbb{R}^d$，可以是任何维度的</p>
<ol>
<li>初始化为0：$W_0 = 0$</li>
<li>高斯增量：$W_t - W_s\sim \mathcal{N}(0, (t-s)I_d), \;(0\le s\le t)$</li>
<li>独立的增量：$W_{t_1} - W_{t_0}, \cdots, W_{t_n} - W_{t_{n-1}}$，$0 \le t_0 \lt t_1 \lt \cdots \lt t_n$ 都是互相独立的，视为随机变量</li>
</ol>
<p>这个独特属性，使得它在任何地方都不可微。</p>
<p>但我们在研究依赖于求导的微分方程。</p>
<p><font color="blue">5. 符号$dX_t$：</font></p>
        </div>

        <!-- Slide 11 -->
        <div class="slide" data-slide="11" data-chapter="第一章 利用随机微分方程的Gen AI">
<p>由于维纳过程不可微，我们换种表达：</p>
$\frac{d}{dt}X_t = u_t(X_t)$

$ \Leftrightarrow \quad \lim_{h \to 0} \frac{X_{t+h} - X_t}{h} = u_t(X_t)$

$ \Leftrightarrow \quad \frac{X_{t+h} - X_t}{h} = u_t(X_t) + R_t(h)$

$ \Leftrightarrow \quad X_{t+h} = X_t + hu_t(X_t) + hR_t(h)$

<p>这里 $R_t(h)$ 是误差项，且 $\lim_{h \to 0} R_t(h) = 0$。这儿想作是泰勒近似。</p>
<p><font color="brown">$dX_t = u_t(X_t)dt + \sigma_tdW_t$</font></p>
<p><font color="brown">$ \Leftrightarrow \quad X_{t+h} = X_t + hu_t(X_t) + \sigma_t(W_{t+h} - W_t) +hR_t(h)$</font></p>
<table>
<thead>
<tr>
<th>ODE</th>
<th>SDE</th>
</tr>
</thead>
<tbody>
<tr>
<td>解是轨迹</td>
<td>解是随机过程，或说随机轨迹</td>
</tr>
<tr>
<td>由向量场定义。</td>
<td>由向量场 和 扩散系数 定义。</td>
</tr>
</tbody>
</table>
<h6 id="222">2.2.2 定理</h6>
<p>SDEs 解的存在性与唯一性定理</p>
<p>如果向量场 $u_t(x)$ 是连续可微的，且其导数有界，并且扩散系数是连续的，那么下面这个随机微分方程</p>
$X_0 = x_0, \quad dX_t = u_t(X_t)dt + \sigma_tdW_t$

<p>存在唯一解。</p>
        </div>

        <!-- Slide 12 -->
        <div class="slide" data-slide="12" data-chapter="第一章 利用随机微分方程的Gen AI">
<h6 id="223-sde-">2.2.3 SDE数值模拟——欧拉-丸山法</h6>
<p>算法2：从一个SDE采样（欧拉-丸山法，Euler-Maruyama method）</p>
<p>输入：向量场$u_t$，步数$n$，扩散系数$\sigma_t$</p>
<ol>
<li>
<p>设 $t = 0$</p>
</li>
<li>
<p>设步长 $h = \frac{1}{n}$</p>
</li>
<li>
<p>设 $X_0 = x_0$</p>
</li>
<li>
<p>对 $i = 1, \cdots, n-1$ 循环：</p>
</li>
</ol>
<p>​        从标准 $d$-维正态分布中采样 $\epsilon \sim \mathcal{N}(0, I_d)$</p>
<p>​    $X_{t+h} = X_t + hu_t(X_t) + \sigma_t \sqrt{h}\epsilon$</p>
<blockquote>
<p>[!NOTE]</p>
<p>$\sqrt{h}\epsilon\sim \mathcal{N}(0, hI_d)$，噪声的方差为$h$。</p>
</blockquote>
<p>​    更新$t$为$t+h$</p>
<ol>
<li>
<p>结束循环</p>
</li>
<li>
<p>返回轨迹：$X_0, X_h, X_{2h}, \cdots, X_1$</p>
</li>
</ol>
<h6 id="224-">2.2.4 示例：奥-乌过程</h6>
<p>Ornstein–Uhlenbeck (OU) 过程是一个<strong>均值回复型的随机过程</strong>，是布朗运动（随机游走）的扩展。它经常用来建模那些<strong>会在长期内回到某个平衡值附近波动</strong>的系统。</p>
        </div>

        <!-- Slide 13 -->
        <div class="slide" data-slide="13" data-chapter="第一章 利用随机微分方程的Gen AI">
$dX_t = -\theta X_t dt + \sigma dW_t$

<p><img src="assets/image-20250714215209895.png" alt="image-20250714215209895" style="zoom:50%;" /></p>
<h6 id="225">2.2.5 生成模型</h6>
<ol>
<li>
<p>扩散模型：$p_{init} \xrightarrow{\text{SDE}} p_{data}$</p>
</li>
<li>
<p>神经网络：是向量场，此处和流模型一样。</p>
</li>
</ol>
<p>​                $u_t^\theta : \mathbb{R}^d \times [0, 1] \to \mathbb{R}^d$，$\theta$ 是网络参数</p>
<ol>
<li>扩散系数：</li>
</ol>
$\sigma_t$
<p>（大多数情况下它是固定的）</p>
<ol>
<li>随机初始条件：</li>
</ol>
$X_0 \sim p_{init}$

<ol>
<li>常微分方程：</li>
</ol>
$dX_t = u_t^\theta(X_t)dt + \sigma_tdW_t$

<ol>
<li>目标：</li>
</ol>
$X_1 \sim p_{data}$
        </div>

        <!-- Slide 14 -->
        <div class="slide" data-slide="14" data-chapter="第二章 构建训练目标">
<h2 id="_1">第二章 构建训练目标</h2>
<p>回顾：</p>
<p><img src="assets/image-20250715133905048.png" alt="image-20250715133905048" style="zoom:50%;" /></p>
<h3 id="_2">第一节 训练模型</h3>
<p>不经过训练，模型的产出“毫无意义” → 我们需要训练向量场 $u_t$</p>
<p>训练 = 找到一组参数，使得：</p>
$\underbrace{X_0 \sim p_{init}}_{\text{从一个初始分布开始，}}$
<p>，   能得到 </p>
<p>在回归或分类任务中，训练目标是标签。</p>
<p>但在这里：没有标签 : (</p>
<p>我们必须推导出一个训练目标。</p>
<h3 id="_3">第二节 构建训练目标</h3>
<p><strong>目的</strong>：推导一个用于训练我们模型的训练目标的公式。</p>
<p>这一节的课程将是技术上最具挑战性的一节！接下来的课程会轻松很多很多。</p>
<p>你不必理解推导过程，但一定要理解以下公式：</p>
$\underbrace{dX_t = u_t^\theta(X_t)dt}_{\text{沿着向量场进行演化，}}$$\underbrace{X_1 \sim p_{data}}_{\text{最终点的分布 = 数据分布}}$
        </div>

        <!-- Slide 15 -->
        <div class="slide" data-slide="15" data-chapter="第二章 构建训练目标">
<p><img src="assets/image-20250715141504078.png" alt="image-20250715141504078" style="zoom:50%;" /></p>
<p>三个条件对象，三个边际对象的公式：</p>
<ul>
<li>条件和边际的概率路径</li>
<li>条件和边际的向量场</li>
<li>条件和边际的得分函数</li>
</ul>
<h4 id="21">2.1 条件和边际概率路径</h4>
<h6 id="211">2.1.1 关键术语</h6>
<ul>
<li>“<font color="blue">Conditional</font>” = “针对单个数据点”</li>
<li>“<font color="blue">Marginal</font>” = “跨数据点分布”</li>
</ul>
<p>Conditional（条件的） 强调的是在某个特定数据点条件下的情况。</p>
<p>Marginal（边际的） 是指考虑整个数据的整体分布，不针对单点。</p>
<p>:mag: <a href="#marginal">“边际”</a></p>
<h6 id="212">2.1.2 概率路径</h6>
<p><font color="blue">概率路径：</font> 从噪声到数据的路径。（噪声和数据的逐步插值）</p>
<p>狄拉克分布（ Dirac distribution）：$z \in \mathbb{R}^d$,  $\delta_z: X \sim \delta_z \Rightarrow X = z$</p>
<blockquote>
<p>这是最简单的一种分布。它是一种<strong>确定性分布</strong>（deterministic distribution）：它在 $x = z$ 处“无限高”，在 $x \ne z$ 的地方为 0，积分为 1。</p>
</blockquote>
        </div>

        <!-- Slide 16 -->
        <div class="slide" data-slide="16" data-chapter="第二章 构建训练目标">
<blockquote>
<p>你可以把它看成一个“浓缩在一个点 $z$ 上的概率分布”，所有质量都集中在 $z$​，没有任何扩散或随机性。从 狄拉克分布中采样的结果就是 $z$ 本身，毫无随机性。</p>
<p>[!TIP]</p>
<p>一开始，大家尝试寻找“从噪声变成数据”的最优路径，比如在纯粹的神经常微分方程（neural ODEs）^[1]^ 中，是不去指定中间过程（即路径上的中间分布）的，人们只是希望模型自己找到一条最佳路径。但扩散模型的一个关键思想，就是<strong>明确地指定从噪声到数据的演化路径</strong>。<br />
而实际上，扩散模型这样 <strong>选择一种路径并坚持使用，是完全可行的</strong>——因为这样可以带来可扩展的训练流程。</p>
<p>[1] Chen, Ricky TQ, et al. "Neural ordinary differential equations." <em>Advances in neural information processing systems</em> 31 (2018).</p>
</blockquote>
<h6 id="213">2.1.3 条件概率路径 $p_t(\cdot \mid z)$</h6>
<p><font color="blue">条件概率路径：$p_t(\cdot \mid z)$</font></p>
<ol>
<li><font color="blue">$p_t(\cdot \mid z)$ 是$\mathbb{R}^d$上的概率分布</font></li>
<li><font color="blue">$p_0(\cdot \mid z) = p_{init}$</font></li>
<li><font color="blue">$p_1(\cdot \mid z) = \delta_z$</font></li>
</ol>
<h6 id="214">2.1.4 例子 —— 高斯概率路径</h6>
$p_t(\cdot \mid z) = N(\alpha_t z, \beta_t^2 I_d)$

$\alpha_t$
<p>和  是所谓的噪声调度器（noise schedulers） </p>
$\beta_t$
        </div>

        <!-- Slide 17 -->
        <div class="slide" data-slide="17" data-chapter="第二章 构建训练目标">
<ul>
<li>$\alpha_0 = 0$，$\alpha_1 = 1$ </li>
<li>$\beta_0 = 1$，$\beta_1 = 0$ </li>
<li>比如常见可以设计为 $\alpha_t = t$，$\beta_t = 1 - t$</li>
</ul>
<p>容易得到，它满足<font color="blue">条件概率路径 $p_t(\cdot \mid z)$</font>的三点要求。</p>
<p>如下图可视化：</p>
<p><img src="assets/image-20250715144434634.png" alt="image-20250715144434634" style="zoom:35%;" /></p>
<h6 id="215">2.1.5 边际概率路径 $p_t$</h6>
<p><font color="blue">边际概率路径：$p_t$</font></p>
$z \sim p_{data}, \; x \sim p_t(\cdot \mid z) \Rightarrow \underbrace{x \sim p_t}_{\text{forget z}}$

<blockquote>
<p>通过条件概率路径 + 数据分布 可以推出 边际概率路径。即：</p>
<p><strong>流模型中时刻  $t$ 的边际分布是对所有初始数据 $z$ 的条件分布 $p_t(X \mid z)$ 的加权平均</strong>，权重由初始数据的分布 $p_{\text{data}}(z)$ 决定。</p>
</blockquote>
<ol>
<li>$p_t(X) = \int p_t(x|z)p_{data}(z)dz$</li>
<li>$p_0 = p_{init}$</li>
<li>$p_1 = p_{data}$</li>
</ol>
        </div>

        <!-- Slide 18 -->
        <div class="slide" data-slide="18" data-chapter="第二章 构建训练目标">
<p>如下图可视化：</p>
<p><img src="assets/image-20250715151652997.png" alt="image-20250715151652997" style="zoom:50%;" /></p>
<h6 id="216">2.1.6 概率路径小结</h6>
<p>条件概率路径 $p_t(\cdot \mid z)$，由 $p_{init}$ 和 一个数据点 $z$ 插值，例子是高斯概率路径$N(\alpha_t z, \beta_t^2 I_d)$，$\alpha_t$从0到1，$\beta_t$从1到0。</p>
<p>边缘化得到</p>
<p>边际概率路径$p_t$，由 $p_{init}$ 和 $p_{data}$ 插值，有公式 $p_t(x) = \int p_t(x| z)p_{data}(z)dz$。</p>
<blockquote>
<p>[!NOTE]</p>
<p>公式 $p_t(X) = \int p_t(x|z)p_{data}(z)dz$ </p>
<p>把 $z$ 给“边缘化掉”了（marginalized out）——所以叫它“边际分布”。</p>
<p>在概率论中，“<strong>边际</strong>”这个词来自于一个常见的操作 —— <strong>从联合分布或条件分布中通过积分“边缘化掉”一些变量</strong>，只保留我们关心的部分。 </p>
</blockquote>
<p>| 条件概率 $p(x \mid z)$ | 给定 $z$，$x$ 的分布          | 先挑定一个初始点，观察它的演化路径 |<br />
| 联合概率 $p(x, z)$     | $x$ 和 $z$ 同时出现的概率     | 所有起点与终点对的联合分布         |<br />
| <strong>边际概率 $p(x)$</strong>    | 不关心 $z$ 时，$x$ 的总体分布 | 所有起点演化后的“总体效果”         |<br />
| 术语                   | 含义                          | 举例                               |<br />
| ---------------------- | ----------------------------- | ---------------------------------- |</p>
        </div>

        <!-- Slide 19 -->
        <div class="slide" data-slide="19" data-chapter="第二章 构建训练目标">
<blockquote>
<p>[!TIP]</p>
<p><a name="marginal">“边际”</a>这个词来自表格“边缘”的历史传统，而不是因为它本身有什么边的含义。从语义角度来说确实不够直观，但它已经成为标准术语。你记住“边际 = 去掉另一个变量后，留下的总概率”就可以了。</p>
<table>
<thead>
<tr>
<th>性别 / 吸烟</th>
<th>吸烟 (Yes)</th>
<th>不吸烟 (No)</th>
<th>总计（边缘）</th>
</tr>
</thead>
<tbody>
<tr>
<td>男 (Male)</td>
<td>30/100 = <strong>0.30</strong></td>
<td>20/100 = <strong>0.20</strong></td>
<td><strong>0.50</strong></td>
</tr>
<tr>
<td>女 (Female)</td>
<td>10/100 = <strong>0.10</strong></td>
<td>40/100 = <strong>0.40</strong></td>
<td><strong>0.50</strong></td>
</tr>
<tr>
<td><strong>总计</strong></td>
<td><strong>0.40</strong></td>
<td><strong>0.60</strong></td>
<td><strong>1.00</strong></td>
</tr>
</tbody>
</table>
<p>如果你觉得难记，大可以在心里把它当作“总分布”或“全局分布”来理解，也没问题 。</p>
</blockquote>
<h4 id="22">2.2 条件和边际向量场</h4>
<h6 id="221">2.2.1 条件向量场</h6>
<blockquote>
<p>[!NOTE]</p>
<p>我们希望有一个ODE，能沿着条件概率路径，从噪声到单个数据点，即 $p_{init} \overset{\text{$p_t(\cdot \mid z)$}}{\underset{\text{ODE}}{\longrightarrow}} \delta_z$ </p>
</blockquote>
        </div>

        <!-- Slide 20 -->
        <div class="slide" data-slide="20" data-chapter="第二章 构建训练目标">
<p>形式化（公式化）表达：</p>
$u_t^{target}(x|z)$
<p>，（  ） </p>
<p>满足</p>
<p><font color="blue">$X_0 \sim p_{init},\quad \frac{d}{dt}X_t = u_t^{target}(x_t|z) \Rightarrow  X_t \sim p_t(\cdot \mid z)$  </font> $(0 \le t \le 1)$</p>
<h6 id="222">2.2.2 例子——条件高斯向量场</h6>
$u_t^{target}(x|z) = (\dot{\alpha_t} - \frac{\dot{\beta_t}}{\beta_t}\alpha_t)z+ \frac{\dot{\beta_t}}{\beta_t}x$
<p>， </p>
<blockquote>
<ul>
<li>$\dot{}$ 在物理中常用于表示时间导数，如速度$\dot{x}$，所以 $\dot{\alpha_t}=\frac{d}{dt}\alpha_t$</li>
<li>这个向量场很简单，就是 $x$ 和 $z$ 的某种加权组合。</li>
</ul>
</blockquote>
<p>这个向量场能沿着高斯概率路径 $p_t(\cdot \mid z) = N(\alpha_t z, \beta_t^2 I_d)$ 从噪声到数据点。</p>
<p>可视化为下图：</p>
<p><img src="assets/image-20250715164951644.png" alt="image-20250715164951644" style="zoom:50%;" /></p>
<p><img src="assets/image-20250715164934805.png" alt="image-20250715164934805" style="zoom:50%;" /></p>
<h6 id="223">2.2.3 边际向量场 &amp; 定理（边缘化技巧）</h6>
<p>边际向量场：</p>
<p><font color="blue">$u_t^{target}(x) = \int u_t^{target}(x|z) \frac{p_t(x|z)p_{data}(z)}{p_t(x)}dz$</font></p>
$\substack{0 \le t \le 1 \\ x,z \in \mathbb{R}^d}$
        </div>

        <!-- Slide 21 -->
        <div class="slide" data-slide="21" data-chapter="第二章 构建训练目标">
$\frac{p_t(x|z)p_{data}(z)}{p_t(x)}$
<p>这个比率，本质上是给定，采用的后验概率，即。</p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>这儿要理解一下，边际概率路径是条件概率路径<font color="brown">按 $p(z)$ 加权</font>，描述数据在（$t$时刻）空间中的整体分布，反映“在哪儿”更可能出现。</p>
<p>向量场描述“怎么走”的方向和速度，是动态且带方向性的。由于当前位置 $x$ 下不同初始条件导致的向量场差异较大，向量场需<font color="brown">按后验概率 $p(z \mid x)$ 加权</font>，才能准确反映当前点的运动趋势。</p>
</blockquote>
<p>这样的边际向量场</p>
<p><a name="theorem">满足</a></p>
<p><font color="blue">$X_0 \sim p_{init}, \quad \frac{d}{dt}X_t = u_t^{target}(x_t) \Rightarrow X_t \sim p_t \quad (0 \le t \le 1)$</font>  $ \Rightarrow X_1 \sim p_{data}$</p>
<blockquote>
<p>[!TIP]</p>
<p>回顾这些边际对象的公式，可以看到研究 条件对象 只是工具，都是为了构建 边际对象 的公式。</p>
</blockquote>
$第一行 \xrightarrow{ 边缘化 }第二行$
<p>：</p>
<p><img src="assets/image-20250715210906306.png" alt="image-20250715210906306" style="zoom:50%;" /></p>
<h6 id="224">2.2.4 连续性方程（延伸知识，用于证明边际向量场的边缘化）</h6>
<p>给定：$X_0 \sim p_{init}, \quad \frac{d}{dt}X_t = u_t(X_t)$ </p>
<p>沿着概率路径 $X_t \sim p_t \quad (0 \le t \le 1)$  （边际是$p_t$）</p>
$x$$z$$p_t(z|x)$
        </div>

        <!-- Slide 22 -->
        <div class="slide" data-slide="22" data-chapter="第二章 构建训练目标">
<p>等价于说：</p>
<p>连续性方程 $\frac{d}{dt}p_t(x) = -div(p_tu_t)(x)$ 成立。（该PDE成立）</p>
$p_t$
<p>在处的时间导数 是由 （）的负散度给出的。</p>
<blockquote>
<p>[!NOTE]</p>
<p>可以理解为 概率密度的变化，取决于该点的负的净流出量。（散度衡量的是<strong>净流出量</strong> 。用于描述 流体是否在某处发散或聚集，正散度表示发散，负散度表示汇聚。）。</p>
<p>换句话说，<font color="blue"> <strong>流出越多，密度下降越快</strong>。</font></p>
</blockquote>
<p><img src="assets/image-20250715220122401.png" alt="image-20250715220122401" style="zoom:50%;" /></p>
<blockquote>
<p>[!CAUTION]</p>
<p>向量场的散度是描述流体“是否在某处发散或聚集”；</p>
<p>散度：$\operatorname{div}(v_t)(x) = \sum_{i=1}^d \frac{\partial (v_t)_i(x)}{\partial x_i}$<br />
</p>
</blockquote>
$x$$p_t u_t$$流量=浓度\times速度$
        </div>

        <!-- Slide 23 -->
        <div class="slide" data-slide="23" data-chapter="第二章 构建训练目标">
<blockquote>
<p>而KL散度是描述两个分布“偏离有多大”。从数学形式和语义来看，它们是两个完全不同的工具。</p>
<p>仅仅因为字面的 “偏离/发散” 符合描述，而使用了相同的术语词汇。</p>
</blockquote>
<p>证明：</p>
$\frac{d}{dt}p_t(x) = \frac{d}{dt}\int p_t(x|z)p_{data}(z)dz = \int \frac{d}{dt}p_t(x|z)p_{data}(z)dz $

$= \int -div(p_t(\cdot\mid z)u_t^{target}(\cdot\mid z))(x)p_{data}(z)dz$

$= -div(\int p_t(x\mid z)u_t^{target}(x\mid z)p_{data}(z))dz$

$= -div(p_t(x)\int u_t^{target}(x\mid z)\frac{p_t(x\mid z) p_{data}(z)}{p_t(x)})dz$

$= -div(p_t u_t^{target})(x)$

<blockquote>
<p>[!TIP]</p>
<p>其实就是利用连续方程：</p>
$\frac{d}{dt}p_t(x|z) = -div(p_t(\cdot|z)u_t^{target}(\cdot|z))(x)$
<p>$\frac{d}{dt}p_t(x) = -div(p_tu_t)(x)$ </p>
</blockquote>
        </div>

        <!-- Slide 24 -->
        <div class="slide" data-slide="24" data-chapter="第二章 构建训练目标">
<blockquote>
<p>的定义</p>
<p>，在推导过程中，得到的$u_t^{target}(x) = \int u_t^{target}(x|z) \frac{p_t(x|z)p_{data}(z)}{p_t(x)}dz$</p>
</blockquote>
<h6 id="225">2.2.5 向量场小结</h6>
<p>条件向量场 $u_t^{target}(x \mid z)$，其ODE沿着条件路径，例子是条件高斯向量场$(\dot{\alpha_t} - \frac{\dot{\beta_t}}{\beta_t}\alpha_t)z+ \frac{\dot{\beta_t}}{\beta_t}x$</p>
<p>边缘化得到</p>
<p>边际向量场$u_t^{target}(x)$，其ODE沿着边际路径，有公式 $u_t^{target}(x) = \int u_t^{target}(x|z) \frac{p_t(x|z)p_{data}(z)}{p_t(x)}dz$</p>
<h4 id="23">2.3 条件和边际得分函数（扩散模型）</h4>
<h6 id="231">2.3.1 条件和边际得分</h6>
<p>条件得分：</p>
<p><font color="blue">$\nabla_x \log p_t(x|z)$</font>，即<font color="blue">条件概率路径的对数似然的梯度</font></p>
<p>边际得分：</p>
<p><font color="blue">$\nabla \log p_t(x)$</font>，即<font color="blue">边际概率路径的对数似然的梯度</font></p>
<p>公式：根据链式法则</p>
        </div>

        <!-- Slide 25 -->
        <div class="slide" data-slide="25" data-chapter="第二章 构建训练目标">
<p><font color="blue">$\nabla \log p_t(x) = \frac{\nabla p_t(x)}{p_t(x)} = \frac{\nabla \int p_t(x|z)p_{data}(z)dz}{p_t(x)}$</font></p>
<p><font color="blue">$= \frac{ \int \nabla p_t(x|z)p_{data}(z)dz}{p_t(x)} = \int \nabla \log p_t(x|z) \frac{ p_t(x|z)p_{data}(z)}{p_t(x)}dz$</font>，发现又是构造出<font color="blue">后验概率加权积分</font>的形式。</p>
<blockquote>
$\because \nabla \log p_t(x|z) = \frac{\nabla p_t(x|z)}{p_t(x|z)}$
</blockquote>
<h6 id="232">2.3.2 例子——高斯得分</h6>
<p>一个高斯概率路径对应的高斯得分：</p>
$\nabla_x \log p_t(x|z) = -\frac{x - \alpha_t z}{\beta_t^2}$

<blockquote>
<p>由正太分布的概率密度函数</p>
$p(x) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x - \mu)^\top \Sigma^{-1} (x - \mu) \right)$
<p>代入  $\boldsymbol{\Sigma} = \beta_t^2 \mathbf{I}$</p>
$p(\mathbf{x}) = \frac{1}{(2\pi \beta_t^2)^{d/2}} \exp\left( -\frac{1}{2\beta_t^2} \left\| \mathbf{x} - \alpha_t \mathbf{z} \right\|^2 \right)$
</blockquote>
        </div>

        <!-- Slide 26 -->
        <div class="slide" data-slide="26" data-chapter="第二章 构建训练目标">
<blockquote>
<p>推导得到。</p>
</blockquote>
<h6 id="233-sde">2.3.3 定理（SDE扩展的技巧）</h6>
<p>对于任意$\sigma_t \ge 0$，</p>
$X_0 \sim p_{init}, \quad dX_t = [u_t^{target}(X_t) + \frac{\sigma_t^2}{2}\textcolor{blue}{\nabla \log p_t(x_t)}]dt + \sigma_tdW_t$

<blockquote>
<p>[!NOTE]</p>
<p>得分函数 本质上就是 我们需要应用的<font color="blue">校正项</font>。</p>
<p><strong>得分函数校正了“随机扩散轨迹”的方向</strong>，让它向数据靠近。</p>
</blockquote>
$ \Rightarrow X_t \sim p_t \quad (0 \le t \le 1) \Rightarrow X_1 \sim p_{data}$

<p>:mag: <a href="#theorem">流模型</a> 其实就能达到这个目标，所以现在50%的模型都纯流模型。所以我们优先掌握流模型，扩散模型只是其扩展。</p>
<blockquote>
<p><font color="brown">流模型是基础。</font>  扩散模型更多像一种实践经验，在流模型基础上，通过实验发现加各种扩散系数的噪声，生成效果是否会改善。</p>
</blockquote>
<h4 id="24">2.4 总结</h4>
<p>后续课程会学习到：</p>
<p>模型训练 就是 训练 $u_t^{target}$ 这个对象，或$\nabla \log p_t(x)$这个对象。</p>
<p><img src="assets/image-20250715141504078.png" alt="image-20250715141504078" style="zoom:50%;" /></p>
        </div>

        <!-- Slide 27 -->
        <div class="slide" data-slide="27" data-chapter="第二章 构建训练目标">
<p><img src="assets/image-20250715151929759.png" alt="image-20250715151929759" style="zoom:50%;" /></p>
<p><img src="assets/image-20250715151949401.png" alt="image-20250715151949401" style="zoom:50%;" /></p>
        </div>

        <!-- Slide 28 -->
        <div class="slide" data-slide="28" data-chapter="第三章 训练流模型和扩散模型">
<h2 id="_1">第三章 训练流模型和扩散模型</h2>
<p>回顾：</p>
<p><img src="assets/image-20250715133905048.png" alt="image-20250715133905048" style="zoom:50%;" /></p>
<p><img src="assets/image-20250715151929759.png" alt="image-20250715151929759" style="zoom:50%;" /></p>
<p><img src="assets/image-20250715151949401.png" alt="image-20250715151949401" style="zoom:50%;" /></p>
<blockquote>
<p>[!NOTE]</p>
<p>知识小灶：</p>
<ul>
<li>归一化流：直接学映射函数</li>
</ul>
<p>像修一条高速公路。你得明确规划每个路口（映射函数），并且每段必须符合标准设计（可逆、结构简单、Jacobian 可计算）。</p>
<ul>
<li>流匹配：学导数（vector field）<br />
  像使用GPS。你不关心路具体长什么样，只要告诉我每个时刻往哪个方向走（向量场），然后用 ODE 把路径积分出来，就能从出发点走到终点。</li>
</ul>
</blockquote>
<h4 id="31">3.1 训练算法</h4>
<p>我们将 边际向量场、边际得分函数 转化为 两种算法：<font color="brown">流匹配</font> 与 <font color="brown">得分匹配</font>。</p>
        </div>

        <!-- Slide 29 -->
        <div class="slide" data-slide="29" data-chapter="第三章 训练流模型和扩散模型">
<p>这将是训练算法，用于学习这两个对象。</p>
<p><img src="assets/image-20250716120032690.png" alt="image-20250716120032690" style="zoom:50%;" /></p>
<h4 id="32">3.2 流匹配</h4>
$u_t^\theta$
<p>(: parameters)</p>
<p>目标</p>
$u_t^\theta \approx u_t^{target}$

<h6 id="321">3.2.1 流匹配损失</h6>
<p><font color="blue">$L_{fm} (\theta)= \mathbb{E}[\left \|  u_t^\theta(x) - u_t^{target}(x)\right \|^2 ]$，</font></p>
<blockquote>
<p>✓ Minimizer ✗ Tractable</p>
<p>为什么不易处理呢？ 因为我们无法评估这一点，边际向量场是一个（边缘化）积分，批量进行计算很困难。</p>
</blockquote>
$t \sim \mathcal{U}(0, 1)$
<p>， 在  区间均匀采样。</p>
$z \sim p_{data}$
<p>， 通过dataloader从数据集中随机采样。</p>
$x \sim p_t(\cdot \mid z)$
<p>， 从条件概率路径采样。</p>
<blockquote>
<p>[!NOTE]</p>
<p>这里流匹配损失函数很直观，就是两个对象之间的均方误差。取期望值就是在所有采样样本$(x, t)$上做<code>torch.mean</code>，这是平时实现损失函数常用的。</p>
</blockquote>
$\theta$$t$$[0, 1]$$z$$x$
        </div>

        <!-- Slide 30 -->
        <div class="slide" data-slide="30" data-chapter="第三章 训练流模型和扩散模型">
<h6 id="322">3.2.2 条件流匹配损失</h6>
<p><font color="blue">$L_{cfm} (\theta)= \mathbb{E}[\left \|  u_t^\theta(x) - u_t^{target}(x \mid z)\right \|^2 ]$，</font></p>
<blockquote>
<p>? Minimizer ✓ Tractable</p>
<p>最小化这个对象是否有意义？因为条件向量场不是真的有用，我们不想生成单个数据点，而是想生成整个数据分布。</p>
<p>但接下来我们会证明，最小化条件流匹配损失，能够达到我们的目标。</p>
</blockquote>
$t \sim \mathcal{U}(0, 1)$
<p>，，</p>
<h6 id="323"><a name="Theorem4"> 3.2.3 定理</a></h6>
<p><font color="blue"> $L_{fm}(\theta) = L_{cfm}(\theta) + C$ </font>，for $C \lt 0$ independent of $\theta$</p>
<blockquote>
<p>[!TIP]</p>
<p>$C$ 是与$\theta$无关的常数，不会影响梯度下降方向。从 <strong>优化角度</strong>来看，优化 $L_{cfm}(\theta)$ 与优化 $L_{fm}(\theta)$ 是等价的</p>
</blockquote>
<p><img src="assets/image-20250716142449141.png" alt="image-20250716142449141" style="zoom:35%;" /></p>
$\theta$
<p>的值不重要，我们不关心神经网络具体参数值什么，重要的是最小化器（Minimizer），我们记为 。</p>
$\Rightarrow$
<p>①  对于  的最小化器 ：</p>
$z \sim p_{data}$$x \sim p_t(\cdot \mid z)$$\theta^*$$L_{cfm}$$\theta^*$$u_t^{\theta^*} = u_t^{target}$
        </div>

        <!-- Slide 31 -->
        <div class="slide" data-slide="31" data-chapter="第三章 训练流模型和扩散模型">
$\Rightarrow$
<p>②  </p>
<p>​   $\Rightarrow$  SGD（随机梯度下降）是相同的。</p>
<h6 id="324">3.2.4 算法（通用）</h6>
<p>算法3：流匹配训练过程（通用）</p>
<p>输入：一个样本 $z \sim p_{data}$ 数据集，神经网络 $u_t^{\theta}$</p>
<p>对每个最小批次（mini-batch）的数据循环：</p>
<p>​   采样$z \sim p_{data}$</p>
<p>​   采样$t \sim \mathcal{Unif}_{[0, 1]}$</p>
<p>​   采样 $x \sim p_t(\cdot \mid z)$</p>
<p>​   计算损失 $L (\theta)= \left \|  u_t^\theta(x) - u_t^{target}(x \mid z)\right \|^2$</p>
<p>​      （选择一种优化器）梯度下降更新模型参数</p>
<p>循环结束</p>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>流和扩散模型的强大之处就在于只需要最小化简单的均方误差。例如，GANs会有一个最小最大优化程序，比这复杂得多。</li>
<li>这儿条件概率路径 $p_t(\cdot \mid z)$、条件向量场$u_t^{target}(x \mid z)$ 如上节课讲的，是我们选择的一组，它可以完成我们想要的工作。这是一个设计选择，也有很多其他选择，接下来的课程我们实际上会看到一组新的选择。</li>
</ul>
</blockquote>
<h6 id="325">3.2.5 例子——高斯概率路径的$L_{cfm}$</h6>
$\nabla_\theta L_{cfm}(\theta) = \nabla_\theta L_{fm}(\theta)$
        </div>

        <!-- Slide 32 -->
        <div class="slide" data-slide="32" data-chapter="第三章 训练流模型和扩散模型">
<p>回顾：</p>
$p_t(\cdot \mid z) = N(\alpha_t z, \beta_t^2 I_d)$
<p>，</p>
$u_t^{target}(x|z) = (\dot{\alpha_t} - \frac{\dot{\beta_t}}{\beta_t}\alpha_t)z+ \frac{\dot{\beta_t}}{\beta_t}x$

<p>继续推：</p>
$\varepsilon\sim \mathcal{N}(0, I_d) \Rightarrow \alpha_tz + \beta_t\varepsilon \overset{\text{define}}{=} x \sim p_t(\cdot \mid z)$

$\Rightarrow$

$L_{cfm} (\theta)= \mathbb{E}[\left \|  u_t^\theta(x) - (\dot{\alpha_t} - \frac{\dot{\beta_t}}{\beta_t}\alpha_t)z - \frac{\dot{\beta_t}}{\beta_t}x \right \|^2 ]$

<blockquote>
<p>$t \sim \mathcal{U}(0, 1)$，$z \sim p_{data}$，$x\sim \mathcal{N}(\alpha_t z, \beta_t^2 I_d)$</p>
<p>代入 $x = \alpha_tz + \beta_t\varepsilon$  再做些代数：</p>
</blockquote>
$= \mathbb{E}[\left \|  u_t^\theta(\alpha_tz + \beta_t\varepsilon) - (\dot{\alpha_t} - \frac{\dot{\beta_t}}{\beta_t}\alpha_t)z - \frac{\dot{\beta_t}}{\beta_t}(\alpha_tz + \beta_t\varepsilon) \right \|^2]$
        </div>

        <!-- Slide 33 -->
        <div class="slide" data-slide="33" data-chapter="第三章 训练流模型和扩散模型">
$ = \mathbb{E}[\left \|  u_t^\theta(\alpha_tz + \beta_t\varepsilon) - (\dot{\alpha_t}z + \dot{\beta_t}\varepsilon) \right \|^2 ]$

<p>对于指定的条件路径的实例： $\alpha_t = t, \quad \beta_t = 1-t$，</p>
<blockquote>
<p>[!TIP]</p>
<p><font color="brown">Cond OT path</font></p>
<p>选择的 $\alpha_t = t, \quad \beta_t = 1-t$ 这样一条路径有特定的名称，就是所谓的<font color="brown">条件最优传输路径</font>（ Conditional Optimal Transport path）。</p>
</blockquote>
<p>有：$\dot{\alpha_t} = 1, \dot{\beta_t} = -1$</p>
$\therefore L_{cfm} (\theta)= \mathbb{E}[\left \|  u_t^\theta(tz + (1 - t)\varepsilon) - (z -\varepsilon) \right \|^2 ] $

<blockquote>
<p>非常简单吧，无法想象一个更简单的训练算法了。</p>
</blockquote>
<p><img src="assets/image-20250719225950090.png" alt="image-20250719225950090" style="zoom:35%;" /></p>
<h6 id="326-ot">3.2.6 算法（OT）</h6>
<p><strong>Flow Matching Training for CondOT path</strong></p>
<p><a name="algorithm4">算法4</a>：流匹配训练过程（最优传输路径）</p>
<p>输入：一个样本 $z \sim p_{data}$ 数据集，神经网络 $u_t^{\theta}$</p>
        </div>

        <!-- Slide 34 -->
        <div class="slide" data-slide="34" data-chapter="第三章 训练流模型和扩散模型">
<p>对每个最小批次（mini-batch）的数据循环：</p>
<p>​   采样$z \sim p_{data}$</p>
<p>​   采样$t \sim \mathcal{Unif}_{[0, 1]}$</p>
<p>​   采样噪声 $\varepsilon \sim \mathcal{N}(0, I_d)$</p>
<p>​   设 $x = tz + (1-t)\varepsilon$</p>
<p>​   计算损失 $L (\theta)= \left \|  u_t^\theta(x) - (z - \varepsilon)\right \|^2$</p>
<p>​      （选择一种优化器）梯度下降更新模型参数</p>
<p>循环结束</p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>让我们欣赏下该算法的简单：</p>
<p>我们在$z$ 和 $\varepsilon$ 之间的直线上取一个点，我们将其插入神经网络，然后神经网络本质上会学习预测数据和噪声两点之间的差异$z - \epsilon$。</p>
<p>[!TIP]</p>
<p>思考：为了训练更稳定，采样$t$可以随训练进度动态调整，从接近1 → 接近0 的分布。因为$t=0$时只能访问噪声的信息，$t=1$时可以访问完整的数据点。所以随着时间推移，你将需要预测同样的东西，但是拥有的信息是不同的。</p>
</blockquote>
        </div>

        <!-- Slide 35 -->
        <div class="slide" data-slide="35" data-chapter="第三章 训练流模型和扩散模型">
<blockquote>
<p>物理直觉：</p>
<p>本质上是在预测给定噪声、数据点和路径，在这条路径上的某个地方，你需要预测当前的速度。但在直线路径（OT）中，速度只是一个差值，两点间的向量。</p>
<p>而扩散模型走的是非直线，就像一些所谓的方差保持路径或方差爆炸路径。 </p>
</blockquote>
<p>这可不是什么奇特的、太简单的算法，MovieGen（Meta）、Stable Diffusion 3（Stability AI）就是用的该<a href="#algorithm4">算法</a>。</p>
<h6 id="327">3.2.7 证明</h6>
<p>对<a href="#Theorem4">定理</a>：$L_{fm}(\theta) = L_{cfm}(\theta) + C$ 的证明：</p>
$\left\| a -b   \right\|^2 = \left\| a \right\|^2 - 2a^Tb + \left\| b \right\|^2$
<p>,   </p>
$L_{fm} (\theta)= \mathbb{E}[\left \|  u_t^\theta(x) - u_t^{target}(x)\right \|^2 ] =\mathbb{E}[\left \|  u_t^\theta(x) \right\|^2 -2u_t^\theta(x)^Tu_t^{target}(x) + \left \| u_t^{target}(x)\right \|^2 ]$

$L_{cfm} (\theta)= \mathbb{E}[\left \|  u_t^\theta(x) - u_t^{target}(x \mid z)\right \|^2 ] =\mathbb{E}[\left \|  u_t^\theta(x) \right\|^2 -2u_t^\theta(x)^Tu_t^{target}(x \mid z) + \left \| u_t^{target}(x \mid z)\right \|^2 ]$

$(a,b \in \mathbb{R}^d)$
        </div>

        <!-- Slide 36 -->
        <div class="slide" data-slide="36" data-chapter="第三章 训练流模型和扩散模型">
<blockquote>
<p>然后  $\left \| u_t^{target}(x)\right \|^2$ 和 $\left \| u_t^{target}(x \mid z)\right \|^2$ 是和 $\theta$ 无关的常数，可以消掉，$\left \|  u_t^\theta(x) \right\|^2$ 项相同，也可以消掉。</p>
</blockquote>
<p>转变成了证明  $2u_t^\theta(x)^Tu_t^{target}(x)$ 与 $2u_t^\theta(x)^Tu_t^{target}(x\mid z)$ 的期望相同。 在课堂笔记中，对此进行了详细证明，这儿的点积是线性的，这里就略过了。</p>
<h6 id="328">3.2.8 采样算法</h6>
<p>我们如何从刚刚训练好的流模型中采样（生成对象）呢？</p>
<p>参加<a href="#algorithm1">算法1 </a>ODE数值模拟——欧拉法</p>
<blockquote>
<p>[!TIP]</p>
<p>问：人们是否使用欧拉法？</p>
<p>答：最初是的，但现在人们最想要最小化神经网络的预测次数，也就是数值模拟中有多少步。所以关心效率时，人们通常使用高阶ODE<font color="brown">求解器</font>。</p>
<ul>
<li>Euler 方法（欧拉法）是最基本的 ODE 数值求解器之一，属于一阶（first-order）ODE solver。</li>
<li>“higher-order” 指的是方法的收敛阶（order of accuracy），也就是它逼近真实解的速度。</li>
</ul>
</blockquote>
<h4 id="33">3.3 得分匹配</h4>
<p>回顾：</p>
<p>边际得分函数：</p>
<p><font color="blue">$\nabla \log p_t(x) = \int \nabla \log p_t(x|z) \frac{ p_t(x|z)p_{data}(z)}{p_t(x)}dz$</font>，是<font color="blue">后验概率加权积分</font>的形式。</p>
        </div>

        <!-- Slide 37 -->
        <div class="slide" data-slide="37" data-chapter="第三章 训练流模型和扩散模型">
<p>定理（SDE扩展的技巧）：</p>
<p>对于任意$\sigma_t \ge 0$，</p>
$X_0 \sim p_{init}, \quad dX_t = [u_t^{target}(X_t) + \frac{\sigma_t^2}{2}\textcolor{blue}{\nabla \log p_t(x_t)}]dt + \sigma_tdW_t$

<blockquote>
<p>[!NOTE]</p>
<p>得分函数 本质上就是 我们需要应用的<font color="blue">校正项</font>。</p>
<p><strong>得分函数校正了“随机扩散轨迹”的方向</strong>，让它向数据靠近。</p>
</blockquote>
$ \Rightarrow X_t \sim p_t \quad (0 \le t \le 1) \Rightarrow X_1 \sim p_{data}$

<h6 id="331">3.3.1 得分匹配损失</h6>
<p>得分匹配网络：</p>
$s_t^\theta$
<p>(：parameters )</p>
<p>目标： </p>
$s_t^\theta \approx \nabla \log p_t$

<p>由于边际得分函数和边际向量场的（边缘化）公式非常类似，这儿的推导也是类似的，即将证明：$\mathcal{L}_{sm}(\theta) = \mathcal{L}_{dsm}(\theta) + C$</p>
<blockquote>
<p>[!NOTE]</p>
</blockquote>
$\theta$
        </div>

        <!-- Slide 38 -->
        <div class="slide" data-slide="38" data-chapter="第三章 训练流模型和扩散模型">
<blockquote>
<p>人们习惯将条件得分匹配损失，称作<font color="blue">去噪得分匹配损失</font>。实则是一回事。</p>
</blockquote>
<h6 id="332">3.3.2 去噪得分匹配损失</h6>
<p><font color="blue">$L_{sm} (\theta)= \mathbb{E}_{t,z,x}[\left \|  s_t^\theta(x) - \nabla \log p_t(x) \right \|^2 ]$，</font></p>
<blockquote>
<p>✓ Minimizer ✗ Tractable</p>
</blockquote>
<p><font color="blue">$L_{dsm} (\theta)= \mathbb{E}_{t,z,x}[\left \|  s_t^\theta(x) - \nabla \log p_t(x \mid z) \right \|^2 ]$，</font></p>
<blockquote>
<p>? Minimizer ✓ Tractable</p>
</blockquote>
<h6 id="333">3.3.3 定理</h6>
$\mathcal{L}_{sm}(\theta) = \mathcal{L}_{dsm}(\theta) + C$
<p>，for  independent of </p>
$\Rightarrow$
<p>①  对于  的最小化器 ：</p>
$\Rightarrow$
<p>②  </p>
<p>​   $\Rightarrow$  SGD（随机梯度下降）是相同的。</p>
<h6 id="334">3.3.4 算法（通用）</h6>
<p>算法5：得分匹配训练过程（通用）</p>
<p>输入：一个样本 $z \sim p_{data}$ 数据集，得分网络 $s_t^{\theta}$</p>
$C \lt 0$$\theta$$\mathcal{L}_{dsm}$$\theta^*$$s_t^{\theta^*} = \nabla \log p_t(x)$$\nabla_\theta \mathcal{L}_{dsm}(\theta) = \nabla_\theta L_{sm}(\theta)$
        </div>

        <!-- Slide 39 -->
        <div class="slide" data-slide="39" data-chapter="第三章 训练流模型和扩散模型">
<p>对每个最小批次（mini-batch）的数据循环：</p>
<p>​   采样$z \sim p_{data}$</p>
<p>​   采样$t \sim \mathcal{U}_{[0, 1]}$</p>
<p>​   采样 $x \sim p_t(\cdot \mid z)$</p>
<p>​   计算损失 $\mathcal{L} (\theta)= \left \|  s_t^\theta(x) - \nabla \log p_t(x \mid z)\right \|^2$</p>
<p>​      （选择一种优化器）梯度下降更新模型参数</p>
<p>循环结束</p>
<h6 id="335">3.3.5 例子——高斯概率路径的$\mathcal{L}_{dsm}$</h6>
<p>回顾：</p>
$\nabla_x \log p_t(x|z) = -\frac{x - \alpha_t z}{\beta_t^2}$

$\varepsilon \sim \mathcal{N}(0, I_d) \Rightarrow x = \alpha_t z + \beta_t \varepsilon \sim \mathcal{N}(\alpha_t z, \beta_t^2 I_d)$

<p>继续推：</p>
$\mathcal{L}_{dsm}(\theta) = \mathbb{E}_{t \sim Unif,z \sim p_{data},x \sim p_t(\cdot \mid z)}[\left \|  s_t^\theta(x) + \frac{x - \alpha_t z}{\beta_t^2} \right \|^2 ]$

$= \mathbb{E}_{t \sim Unif,z \sim p_{data},x \sim p_t(\cdot \mid z)}[\left \|  s_t^\theta(\alpha_t z + \beta_t \varepsilon) + \frac{\varepsilon}{\beta_t} \right \|^2 ]$
        </div>

        <!-- Slide 40 -->
        <div class="slide" data-slide="40" data-chapter="第三章 训练流模型和扩散模型">
<blockquote>
<p>[!TIP]</p>
<p>现在你应该理解，为什么它被称为 去噪得分匹配，是因为被发现对于高斯概率路径，我们只是学习预测用于破坏数据点的噪声。</p>
</blockquote>
<h6 id="336">3.3.6 算法（高斯概率路径）</h6>
<p><strong>Score Matching Training for Gaussian probability path</strong></p>
<p>算法6：得分匹配训练过程（高斯概率路径）</p>
<p>输入：一个样本 $z \sim p_{data}$ 数据集，得分网络（或称 噪声预测器） $s_t^{\theta}$ </p>
<p>输入：Schedulers $\alpha_t$, $\beta_t$ with $\alpha_0 = \beta_1 = 0$, $\alpha_1 = \beta_0 = 1$</p>
<p>对每个最小批次（mini-batch）的数据循环：</p>
<p>​   采样$z \sim p_{data}$</p>
<p>​   采样$t \sim \mathcal{Unif}_{[0, 1]}$</p>
<p>​   采样噪声 $\varepsilon \sim \mathcal{N}(0, I_d)$</p>
<p>​   设 $x = \alpha_tz + \beta_t \varepsilon$</p>
<p>​   计算损失 $L (\theta)= \left \|  s_t^\theta(x_t) + \frac{\varepsilon}{\beta_t} \right \|^2$</p>
<p>​      （选择一种优化器）梯度下降更新模型参数</p>
<p>循环结束</p>
        </div>

        <!-- Slide 41 -->
        <div class="slide" data-slide="41" data-chapter="第三章 训练流模型和扩散模型">
<blockquote>
<p>[!NOTE]</p>
<p>值得一提的是，对于较小的$\beta_t$，$\frac{\varepsilon}{\beta_t}$在数值上是不稳定的。所以对于$t$接近1时，损失可能不稳定。</p>
<p>扩散模型研发的早期就意识到这点，并且有一些技巧可以解决。在课堂笔记里有介绍。  但得分匹配是在扩散模型之前就提出的，那时人们反对这种匹配具有高方差。</p>
</blockquote>
<p>问答：</p>
<blockquote>
<p>[!NOTE]</p>
<p>问：我们能否避免同时学习它们（流匹配网络 和 得分匹配网络）？</p>
<p>答：其实我稍后会讲到这个。答案是肯定的。</p>
<p>原则上，对于一般情况，你必须同时学习它们。</p>
<p>但是，在最重要的特定的高斯概率路径，我们可以将它们相互转换。</p>
</blockquote>
        </div>

        <!-- Slide 42 -->
        <div class="slide" data-slide="42" data-chapter="第三章 训练流模型和扩散模型">
<blockquote>
<p>但即使你必须同时学习它们，请记住，我们可以将它们放在同一个网络中，也就是为一个图像的每个像素制作两个输出，所以它的计算成本不会那么高。</p>
</blockquote>
<h6 id="337">3.3.7 采样算法</h6>
<p>扩散模型的随机采样：</p>
<p>我们将训练好的新网络插入到SDE：</p>
$X_0 \sim p_{init}, \quad dX_t = [u_t^{target}(X_t) + \frac{\sigma_t^2}{2}\textcolor{blue}{\nabla \log p_t(x_t)}]dt + \sigma_tdW_t$

$\downarrow$
<p>插入 表示向量场的网络 和 得分网络</p>
$X_0 \sim p_{init}, \quad dX_t = [u_t^{\theta}(X_t) + \frac{\sigma_t^2}{2}s_t^{\theta}(x_t)]dt + \sigma_tdW_t$

<p>经过训练 $\Rightarrow X_t \sim p_t$</p>
<h6 id="338-ddms">3.3.8 去噪扩散模型（DDMs）</h6>
<p><strong>术语</strong></p>
<p>去噪扩散模型 = 高斯概率路径$\mathcal{N}(\alpha_t z, \beta_t^2I_d)$（我们的标准例子）的扩散模型</p>
<p>通用术语中 Terminology（by many people）</p>
<p>去噪扩散模型 = 扩散模型</p>
<blockquote>
<p>也就是说，许多人提到扩散模型，就是指的这个特定的实例。人们会用不同的方式谈论同一件事。</p>
<p>当和你的同事交谈时，他们会用一种完全不同的语言，不要感到困惑。这并不奇怪，因为算法是通过许多不同的方式发现的。</p>
</blockquote>
        </div>

        <!-- Slide 43 -->
        <div class="slide" data-slide="43" data-chapter="第三章 训练流模型和扩散模型">
<p><strong>特殊性质</strong></p>
<p>向量场和得分函数 可以 相互转换。 所以同一个网络即可完成 流匹配 和 得分匹配。</p>
<p><font color="blue">$\because$ 向量场 和 得分函数 都是 $x$ 和 $z$ 的某种加权</font></p>
$\lambda_1 z + \lambda_2 x$
<p>，</p>
<p>经过代数，就能得出它们能相互转换：</p>
<p><img src="assets/image-20250720153027234.png" alt="image-20250720153027234" style="zoom:50%;" /></p>
<p>训练后的边际向量场 可以转换为 得分网络，反之亦然。</p>
<p>即，得分是免费获取的。</p>
<blockquote>
<p>[!TIP]</p>
<p>所以第一代的扩散模型文章，只讨论 得分匹配。 因为它们隐式地依赖于高斯概率路径的去噪扩散模型。然后可以将东西相互转换。</p>
</blockquote>
<p><strong>总结</strong></p>
<p>我们在这里得到一个完整的端到端训练和采样算法。我们有一个通用的模型，可以从数据分布中生成样本。</p>
<p>下周的课程将更加注重应用，我们将讨论针对特定应用可以做出的具体选择：</p>
<ul>
<li>神经网络架构，即$u_t^{\theta}$、$s_t^{\theta}$这些函数具体是什么</li>
</ul>
$\lambda_1' z + \lambda_2' x$
        </div>

        <!-- Slide 44 -->
        <div class="slide" data-slide="44" data-chapter="第三章 训练流模型和扩散模型">
<ul>
<li>基于提示词的条件</li>
<li>图像生成器 或 视频生成器</li>
<li>其他应用：机器人技术、蛋白质设计</li>
</ul>
<p>我们可以将任意分布相互转换，任意的 $p_{init}$ 和 $p_{data}$，所以我们<font color="brown">不仅仅是关心去噪扩散模型（DDMs）</font>。因为DDMs总是从某种高斯噪声作为 $p_{init}$ 开始。</p>
<p>但可能还有其他情况，你的初始分布要有趣得多，而且有很多人在探索这一点。</p>
<p>在图像空间、音频空间以及科学领域，许多时候，你的初始分布本身就很有意义，例如：</p>
<p><img src="assets/image-20250720160712054.png" alt="image-20250720160712054" style="zoom:50%;" /></p>
<blockquote>
<p>我觉得可以预测转换域的东西，定义清楚这个转换，是光流？是高频残差？等等。 有意义的输入则作为参考信息。</p>
</blockquote>
        </div>

        <!-- Slide 45 -->
        <div class="slide" data-slide="45" data-chapter="第四章 构建一个图像生成器">
<h2 id="_1">第四章 构建一个图像生成器</h2>
<p>为了避免混乱，本章只以流模型为例，进行介绍。但所讲述的内容都可扩展到扩散模型。</p>
<p>议程：</p>
<ol>
<li>将生成模型框架从 无条件生成 延伸到 有条件生成。</li>
<li>开发用于条件采样的无分类器引导（classifier-free guidance）方法。</li>
<li>讨论图像生成这一典型案例中的架构选择，并综述当前主流模型。</li>
</ol>
<h4 id="41">4.1 条件生成和引导</h4>
<p>无条件 也可以 称作 无引导（Unguided）</p>
<p>有条件 也可以 称作 有引导（Guided）</p>
<p><img src="assets/image-20250721213103970.png" alt="image-20250721213103970" style="zoom:50%;" /></p>
<h6 id="411">4.1.1 有引导的条件匹配目标函数 $\mathcal{L}_{cfm}^{guided}$</h6>
<p>观察：当 $y$ 固定时，我们实际退化为无引导生成：</p>
<blockquote>
<p>此时 $y$ 可以理解为 超参数。 </p>
</blockquote>
$\mathcal{L}_{cfm}^{guided}(\theta ; y) =\mathbb{E}_{\square}[\left \|  u_t^\theta(x) - u_t^{target}(x \mid z)\right \|^2 ]$
<p>，</p>
$\square = \textcolor{blue}{z \sim p_{data}(z \mid y)}, t \sim \mathcal{U}[0,1), x \sim p_t(x\mid z)$
        </div>

        <!-- Slide 46 -->
        <div class="slide" data-slide="46" data-chapter="第四章 构建一个图像生成器">
<p>观察：当让 $y$ 变化时：</p>
$\mathcal{L}_{cfm}^{guided}(\theta) =\mathbb{E}_{\square}[\left \|  u_t^\theta(x) - u_t^{target}(x \mid z)\right \|^2 ]$
<p>，</p>
$\square = \textcolor{red}{(z, y) \sim p_{data}(z, y)}, t \sim \mathcal{U}[0,1), x \sim p_t(x\mid z)$

<h6 id="412-cfg">4.1.2 无分类器引导（CFG）</h6>
<table>
<thead>
<tr>
<th>名称</th>
<th>定义</th>
<th>是否使用条件？</th>
<th>是否用分类器？</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>无引导生成</strong> (<em>unconditional generation</em>)</td>
<td>不提供任何条件，模型自由生成样本</td>
<td>❌ 否</td>
<td>❌ 否</td>
</tr>
<tr>
<td><strong>有引导生成</strong> (<em>conditional generation</em>)</td>
<td>提供条件（如文本、类别）来控制生成</td>
<td>✅ 是</td>
<td>✅ / ❌</td>
</tr>
<tr>
<td><strong>有分类器引导</strong> (<em>classifier guidance</em>)</td>
<td>使用独立的分类器来引导生成趋向目标类别</td>
<td>✅ 是</td>
<td>✅ 是</td>
</tr>
<tr>
<td><strong>无分类器引导</strong> (<em>classifier-free guidance</em>)</td>
<td>不使用分类器，而是训练一个能同时进行有/无条件生成的模型，在推理时人为混合</td>
<td>✅ 是</td>
<td>❌ 否</td>
</tr>
</tbody>
</table>
        </div>

        <!-- Slide 47 -->
        <div class="slide" data-slide="47" data-chapter="第四章 构建一个图像生成器">
<p><strong>无引导生成</strong>：用于评估模型的基础能力；或生成多样化样本；</p>
<p><strong>有引导生成</strong>：</p>
<ul>
<li><strong>有分类器引导</strong>：早期的方法，但需要训练或提供外部分类器；</li>
</ul>
<p><strong>用另一个分类器告诉模型“图像像不像 $y$”</strong>，然后往“更像 $y$”方向走。</p>
<ul>
<li>本质：<strong>用分类器给的方向来指导生成</strong></li>
<li>实现：要给图像打分，然后反向传播出“变得更像 y”的梯度</li>
<li>
<p>缺点：<strong>慢，要用梯度反传，还要额外训练一个分类器</strong></p>
</li>
<li>
<p><strong>无分类器引导</strong>：现代主流方法，易实现且效果好。</p>
</li>
</ul>
<p><strong>用模型自己预测“有条件”和“没条件”两种方向，然后人为混合</strong>，走向更像 y 的方向。</p>
<ul>
<li>
<p>本质：<strong>用模型自己两个版本的预测结果做“加权”</strong></p>
</li>
<li>
<p>实现：直接：</p>
$\tilde{u}(x \mid y) = (1-w)u(x \mid \emptyset) + w u(x \mid y)$
</li>
<li>
<p>优点：<strong>快！只用一个模型，不用分类器也不用反传梯度</strong></p>
</li>
</ul>
<blockquote>
<p>[!TIP]<br />
</p>
</blockquote>
        </div>

        <!-- Slide 48 -->
        <div class="slide" data-slide="48" data-chapter="第四章 构建一个图像生成器">
<blockquote>
<p>问：为什么不能对有分类器引导也这么加权？</p>
<p>答：因为它的“引导方向”是用分类器反传出来的梯度（方向），<strong>不是一个完整的预测向量场</strong>，你没法跟另一个向量直接相加。<br />
 而 classifier-free guidance 是两个完整的模型输出，当然可以直接加。</p>
<ul>
<li>
<p><strong>无分类器引导（Classifier-Free Guidance）</strong> 是一种 <strong>启发式方法</strong>，通过<strong>线性组合</strong>模型在<strong>有条件</strong>与<strong>无条件</strong>下的预测结果，来构造一个引导向量场，以实现条件控制。</p>
</li>
<li>
<p><strong>有分类器引导（Classifier Guidance）</strong> 是一种 <strong>理论驱动的方法</strong>，直接利用一个额外分类器 $C(x)$ 对条件概率 $p(y \mid x)$ 的梯度 $\nabla_x \log p(y \mid x)$，来增强模型生成结果对条件 $y$ 的符合</p>
</li>
<li>
<p>相当于对于 $\nabla_x \log p(x \mid y) = \nabla_x \log p(x) +  w \nabla_x \log p(y \mid x)$，</p>
</li>
</ul>
<p><font color="brown">有分类器引导的$w \nabla_x \log p(y \mid x)$是实际反向传播分类器获得的。而无分类器引导里只是推导过程中的一个式子，可以继续拆解、推导。</font></p>
</blockquote>
<h6 id="413-cfg">4.1.3 CFG推导</h6>
$\log p(x \mid y) = \log p(x) + \log p(y \mid x) - \log p(y)$

<p>对两边求导有：</p>
$\nabla_x \log p(x \mid y) = \nabla_x \log p(x) + \nabla_x \log p(y \mid x) - \underbrace{\nabla_x \log p(y)}_{\text{= 0}}$
        </div>

        <!-- Slide 49 -->
        <div class="slide" data-slide="49" data-chapter="第四章 构建一个图像生成器">
<p>其中</p>
$\nabla_x \log p(x)$
<p>是<strong>无条件（边际）数据分布</strong>的对数梯度，</p>
$\nabla_x \log p(y \mid x)$
<p>是<strong>条件信息（如分类器）提供的额外梯度</strong>。</p>
<p>实际操作中，直接用上式做采样，往往会弱化条件的引导效果，所以提出了引入一个超参数 $w$ 调节条件梯度的强度：</p>
$\nabla_x \log p(x \mid y) = \nabla_x \log p(x) +  w \nabla_x \log p(y \mid x)$

$= \nabla_x \log p(x) + w(\underbrace{\nabla_x \log p(y)}_{=0} + \nabla_x \log p(x \mid y) - \nabla_x \log p(x))$

$= \nabla_x \log p(x) + w \nabla_x \log p(x \mid y) - w \nabla_x \log p(x)$

$= (1 - w)\nabla_x \log p(x) + w \nabla_x \log p(x \mid y)$

<blockquote>
<p>因为 $p(y)$ 与 $x$ 无关，对 $x$ 的梯度是0。</p>
<p>[!NOTE]</p>
<p>$\nabla_x$ 读作：“对 $x$ 的梯度” 或 “在 $x$ 方向上的梯度”。</p>
<p>符号 $\nabla_x \log p(x)$ 通常称为得分函数（score function），也叫对数梯度。它描述“朝着高概率区域改变 $x$ 的方向和速率的向量”。</p>
</blockquote>
$\therefore \tilde{u}_t(x\mid y) = (1 - w)u_t^{target}(x) + wu_t^{target}(x \mid y)$
        </div>

        <!-- Slide 50 -->
        <div class="slide" data-slide="50" data-chapter="第四章 构建一个图像生成器">
<p>推出 无分类器引导（classifier-free guidance）的 核心思想在向量场形式上的表达。</p>
<blockquote>
<p>我们用有条件和无条件的向量场按比例加权，得到一个“折中”方向，来引导采样过程。</p>
</blockquote>
<ul>
<li>$w = 0$：完全不考虑条件，相当于 <strong>无引导生成</strong>。</li>
<li>$w = 1$：完全使用有条件引导，相当于普通 <strong>条件生成</strong>。</li>
<li>$w > 1$：<strong>放大条件影响力</strong>，常用于生成更契合提示的样本</li>
</ul>
<h6 id="414-cfg">4.1.4 CFG训练</h6>
<p>观察：我们可以将无引导的向量场视为没有任何条件的情况。<br />
 但“没有条件”也是一种条件：</p>
$u_t^{target}(x) = u_t^{target}(x\mid y = \emptyset)$

<p>我们现在可以训练一个单一模型 $u_t^{\theta}(x \mid y)$，其中 $y \in \mathcal{Y} \cup \{\emptyset\}$，通过重复使用$\mathcal{L}_{cfm}^{guided}(\theta)$，并偶尔将 $y$ 设为 $\emptyset$：</p>
$\mathcal{L}_{cfm}^{CFG}(\theta) =\mathbb{E}_{\square}[\left \|  u_t^\theta(x) - u_t^{target}(x \mid z)\right \|^2 ]$
<p>，</p>
$\square = (z, y) \sim p_{data}(z, y), \textcolor{red}{\text{with prob.} \eta , y \leftarrow \emptyset}, t \sim \mathcal{U}[0,1), x \sim p_t(x\mid z)$

<h6 id="415-cfg">4.1.5 CFG采样</h6>
<p>每一步都调用模型两次（无条件 &amp; 有条件），计算量是原来的两倍。但是工程上有些优化方法。</p>
<p>算法——CFG采样过程</p>
<p>输入：训练好的有引导的向量场$u_t^{\theta}(x \mid y)$</p>
        </div>

        <!-- Slide 51 -->
        <div class="slide" data-slide="51" data-chapter="第四章 构建一个图像生成器">
<ol>
<li>选择一个提示词 $y \in \mathcal{Y}$，以及一个 $y = \emptyset$ 用于无引导采样</li>
<li>选择一个引导尺度<font color="brown"> $w \gt 1$ </font></li>
<li>初始化 $X_0 \sim p_{init}$</li>
<li>从$t=0$ 到 $t=1$ 模拟 $dX_t = [(1-w)u_t^{\theta}(X_t \mid \emptyset) + w u_t^{\theta}(X_t \mid y)]dt$ </li>
</ol>
<p><img src="assets/image-20250722223728725.png" alt="image-20250722223728725" style="zoom:50%;" /></p>
<p><img src="assets/image-20250722223949275.png" alt="image-20250722223949275" style="zoom:50%;" /></p>
<h4 id="42">4.2 图像生成的网络架构考量</h4>
<p>对于图像这种高维的对象，多层感知机（MLP）是不够的。</p>
<p>我们将探索两种选择：U-Nets（基于卷积）和 DiT（基于注意力）</p>
<p>留意 引导变量 $y$ 是如何编码、嵌入 和 处理的。</p>
<h6 id="421-u-net">4.2.1 U-Net</h6>
<p>参考<a href="https://diffusion.csail.mit.edu/">实验3</a></p>
<h6 id="422-dit">4.2.2 DiT</h6>
<h6 id="423">4.2.3 潜在空间采样</h6>
        </div>

        <!-- Slide 52 -->
        <div class="slide" data-slide="52" data-chapter="番外篇1 讲座">
<h2 id="1">番外篇1 讲座</h2>
<h4 id="1_1">讲座1 联合匹配</h4>
<p>Adjoint Matching：流匹配模型的奖励微调</p>
<p>动机：在扩散模型（diffusion）和流模型（flow models）上进行 RLHF 的最佳方式是什么？</p>
<p><img src="assets/image-20250727185916596.png" alt="image-20250727185916596" style="zoom:50%;" /></p>
<ul>
<li><strong>预训练的扩散模型或流匹配模型</strong>，用于生成分布 $p^{\text{base}}$</li>
<li><strong>奖励模型 $r(x)$</strong>，通过人工偏好训练，或通过编码条件采样，定义为：<br />
    $r(x) = \log p(o \mid x)$</li>
<li><strong>目标</strong>：修改预训练的扩散模型，使其生成的分布满足<br />
    $p^*(x) \propto p(x) \exp(r(x))$</li>
</ul>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>这是一个标准的 <strong>后验分布调整</strong> 的技巧：从原分布 $p(x)$ 中生成样本，但用 $r(x)$ 来「重加权」：越被人类偏好的样本（高 $r(x)$），越应该被更频繁地生成。</li>
<li>这里使用指数exp(r(x))，一来可以保证非负，二来可以放大高 reward 的样本，三来便于对数形式计算$\log p^*(x) = \log p(x) + r(x)$</li>
</ul>
</blockquote>
<h4 id="2">讲座2 机器人</h4>
        </div>

        <!-- Slide 53 -->
        <div class="slide" data-slide="53" data-chapter="番外篇1 讲座">
<h4 id="3">讲座3 蛋白质设计</h4>
        </div>

        <!-- Slide 54 -->
        <div class="slide" data-slide="54" data-chapter="番外篇2 实验">
<h2 id="2">番外篇2 实验</h2>
<h4 id="1-odessdes">实验1 模拟ODEs和SDEs</h4>
<h4 id="2_1">实验2 流匹配和得分匹配</h4>
<h4 id="3">实验3 条件生成模型</h4>
        </div>

    </div>

    <!-- Progress bar -->
    <div class="progress-container">
        <div class="progress-bar" id="progressBar"></div>
    </div>

    <!-- Footer (left bottom) -->
    <div class="footer-text">Notes by Devonn</div>

    <!-- Page number (right bottom) -->
    <div class="page-number">
        <span id="currentSlide">1</span> / <span id="totalSlides">54</span>
    </div>

    <!-- Navigation hint -->
    <div class="nav-hint">
        <kbd>←</kbd> <kbd>→</kbd> 或 <kbd>空格</kbd> 导航
    </div>

    <!-- TOC Icon -->
    <div class="toc-icon" id="tocIcon" title="目录">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <line x1="3" y1="6" x2="21" y2="6"></line>
            <line x1="3" y1="12" x2="21" y2="12"></line>
            <line x1="3" y1="18" x2="21" y2="18"></line>
        </svg>
    </div>

    <!-- TOC Panel -->
    <div class="toc-panel" id="tocPanel">
        <div class="toc-header">
            <h3>目录</h3>
            <button class="toc-close" id="tocClose">×</button>
        </div>
        <div class="toc-content">
        <div class="toc-item" data-level="1" data-slide="1">
            <span class="toc-title">流匹配 和 扩散模型</span>
        </div>
        <div class="toc-item" data-level="2" data-slide="2">
            <span class="toc-title">第零章 学习资源</span>
        </div>
        <div class="toc-item" data-level="2" data-slide="3">
            <span class="toc-title">第一章 利用随机微分方程的Gen AI</span>
        </div>
        <div class="toc-item" data-level="3" data-slide="3">
            <span class="toc-title">第一节：从生成到采样</span>
        </div>
        <div class="toc-item" data-level="3" data-slide="3">
            <span class="toc-title">第二节 流模型与扩散模型</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="4">
            <span class="toc-title">2.1 流模型</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="9">
            <span class="toc-title">2.2 扩散模型</span>
        </div>
        <div class="toc-item" data-level="2" data-slide="14">
            <span class="toc-title">第二章 构建训练目标</span>
        </div>
        <div class="toc-item" data-level="3" data-slide="14">
            <span class="toc-title">第一节 训练模型</span>
        </div>
        <div class="toc-item" data-level="3" data-slide="14">
            <span class="toc-title">第二节 构建训练目标</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="15">
            <span class="toc-title">2.1 条件和边际概率路径</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="19">
            <span class="toc-title">2.2 条件和边际向量场</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="24">
            <span class="toc-title">2.3 条件和边际得分函数（扩散模型）</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="26">
            <span class="toc-title">2.4 总结</span>
        </div>
        <div class="toc-item" data-level="2" data-slide="28">
            <span class="toc-title">第三章 训练流模型和扩散模型</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="28">
            <span class="toc-title">3.1 训练算法</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="29">
            <span class="toc-title">3.2 流匹配</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="36">
            <span class="toc-title">3.3 得分匹配</span>
        </div>
        <div class="toc-item" data-level="2" data-slide="45">
            <span class="toc-title">第四章 构建一个图像生成器</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="45">
            <span class="toc-title">4.1 条件生成和引导</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="51">
            <span class="toc-title">4.2 图像生成的网络架构考量</span>
        </div>
        <div class="toc-item" data-level="2" data-slide="52">
            <span class="toc-title">番外篇1 讲座</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="52">
            <span class="toc-title">讲座1 联合匹配</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="52">
            <span class="toc-title">讲座2 机器人</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="53">
            <span class="toc-title">讲座3 蛋白质设计</span>
        </div>
        <div class="toc-item" data-level="2" data-slide="54">
            <span class="toc-title">番外篇2 实验</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="54">
            <span class="toc-title">实验1 模拟ODEs和SDEs</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="54">
            <span class="toc-title">实验2 流匹配和得分匹配</span>
        </div>
        <div class="toc-item" data-level="4" data-slide="54">
            <span class="toc-title">实验3 条件生成模型</span>
        </div>
        </div>
    </div>

    <!-- Chapter navigation (progress bar) -->
    <div class="chapter-nav">
        <span class="chapter" data-level="1" data-slide="1">流匹配 和 扩散模型</span>
        <span class="separator">|</span>
        <span class="chapter" data-level="2" data-slide="2">第零章 学习资源</span>
        <span class="separator">|</span>
        <span class="chapter" data-level="2" data-slide="3">第一章 利用随机微分方程的Gen AI</span>
        <span class="separator">|</span>
        <span class="chapter" data-level="2" data-slide="14">第二章 构建训练目标</span>
        <span class="separator">|</span>
        <span class="chapter" data-level="2" data-slide="28">第三章 训练流模型和扩散模型</span>
        <span class="separator">|</span>
        <span class="chapter" data-level="2" data-slide="45">第四章 构建一个图像生成器</span>
        <span class="separator">|</span>
        <span class="chapter" data-level="2" data-slide="52">番外篇1 讲座</span>
        <span class="separator">|</span>
        <span class="chapter" data-level="2" data-slide="54">番外篇2 实验</span>
    </div>


    <script>
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        let currentSlide = 1;
        const chapters = [{"title": "流匹配 和 扩散模型", "slide": 1, "level": 1},{"title": "第零章 学习资源", "slide": 2, "level": 2},{"title": "第一章 利用随机微分方程的Gen AI", "slide": 3, "level": 2},{"title": "第二章 构建训练目标", "slide": 14, "level": 2},{"title": "第三章 训练流模型和扩散模型", "slide": 28, "level": 2},{"title": "第四章 构建一个图像生成器", "slide": 45, "level": 2},{"title": "番外篇1 讲座", "slide": 52, "level": 2},{"title": "番外篇2 实验", "slide": 54, "level": 2}];

        // Update chapter navigation highlighting (flattened structure)
        function updateChapterNav(slideNum) {
            const chapterElements = document.querySelectorAll('.chapter-nav .chapter');
            if (chapterElements.length === 0) return;

            // Find which chapter this slide belongs to
            let activeIndex = -1;
            for (let i = 0; i < chapters.length; i++) {
                const nextSlide = i + 1 < chapters.length ? chapters[i + 1].slide : Infinity;
                if (slideNum >= chapters[i].slide && slideNum < nextSlide) {
                    activeIndex = i;
                    break;
                }
            }

            // Update active state
            chapterElements.forEach((el, index) => {
                if (index === activeIndex) {
                    el.classList.add('active');
                } else {
                    el.classList.remove('active');
                }
            });
        }

        // Update TOC active item
        function updateTocActive(slideNum) {
            const tocItems = document.querySelectorAll('.toc-item');
            if (tocItems.length === 0) return;

            let activeIndex = -1;
            for (let i = 0; i < chapters.length; i++) {
                const nextSlide = i + 1 < chapters.length ? chapters[i + 1].slide : Infinity;
                if (slideNum >= chapters[i].slide && slideNum < nextSlide) {
                    activeIndex = i;
                    break;
                }
            }

            tocItems.forEach((item, index) => {
                if (index === activeIndex) {
                    item.classList.add('active');
                } else {
                    item.classList.remove('active');
                }
            });
        }

        // Navigate to specific slide
        function goToSlide(slideNum) {
            if (slideNum < 1 || slideNum > totalSlides) return;
            updateSlide(slideNum);
        }

        // Update display
        function updateSlide(newSlide) {
            if (newSlide < 1 || newSlide > totalSlides) return;

            const current = document.querySelector('.slide.active');
            const next = document.querySelector(`[data-slide="${newSlide}"]`);

            if (current) {
                current.classList.remove('active');
                current.classList.add('exit');
                setTimeout(() => current.classList.remove('exit'), 500);
            }

            if (next) {
                next.classList.add('active');
            }

            currentSlide = newSlide;
            document.getElementById('currentSlide').textContent = currentSlide;
            document.getElementById('progressBar').style.width =
                ((currentSlide / totalSlides) * 100) + '%';

            updateChapterNav(currentSlide);
            updateTocActive(currentSlide);
        }

        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            switch(e.key) {
                case 'ArrowRight':
                case ' ':
                case 'PageDown':
                    e.preventDefault();
                    updateSlide(currentSlide + 1);
                    break;
                case 'ArrowLeft':
                case 'PageUp':
                    e.preventDefault();
                    updateSlide(currentSlide - 1);
                    break;
                case 'Home':
                    e.preventDefault();
                    updateSlide(1);
                    break;
                case 'End':
                    e.preventDefault();
                    updateSlide(totalSlides);
                    break;
                case 'Escape':
                    // Close TOC if open
                    const tocPanel = document.getElementById('tocPanel');
                    if (tocPanel && tocPanel.classList.contains('active')) {
                        tocPanel.classList.remove('active');
                        document.getElementById('tocIcon').style.display = 'flex';
                    }
                    break;
            }
        });

        // Touch navigation
        let touchStartX = 0;
        let touchEndX = 0;

        document.addEventListener('touchstart', (e) => {
            touchStartX = e.changedTouches[0].screenX;
        });

        document.addEventListener('touchend', (e) => {
            touchEndX = e.changedTouches[0].screenX;
            const diff = touchStartX - touchEndX;
            if (Math.abs(diff) > 50) {
                if (diff > 0) {
                    updateSlide(currentSlide + 1);
                } else {
                    updateSlide(currentSlide - 1);
                }
            }
        });

        // Fullscreen toggle
        function toggleFullscreen() {
            if (!document.fullscreenElement) {
                document.documentElement.requestFullscreen();
            } else {
                document.exitFullscreen();
            }
        }

        // TOC Icon click handler
        const tocIcon = document.getElementById('tocIcon');
        const tocPanel = document.getElementById('tocPanel');
        const tocClose = document.getElementById('tocClose');

        if (tocIcon && tocPanel) {
            tocIcon.addEventListener('click', () => {
                tocPanel.classList.add('active');
                tocIcon.style.display = 'none';
            });
        }

        if (tocClose && tocPanel) {
            tocClose.addEventListener('click', () => {
                tocPanel.classList.remove('active');
                tocIcon.style.display = 'flex';
            });
        }

        // Chapter navigation click handlers
        document.querySelectorAll('.chapter-nav .chapter').forEach((chapter) => {
            chapter.addEventListener('click', () => {
                const targetSlide = parseInt(chapter.getAttribute('data-slide'));
                if (targetSlide && targetSlide > 0) {
                    goToSlide(targetSlide);
                }
            });
        });

        // TOC item click handlers
        document.querySelectorAll('.toc-item').forEach((item) => {
            item.addEventListener('click', () => {
                const targetSlide = parseInt(item.getAttribute('data-slide'));
                if (targetSlide && targetSlide > 0) {
                    goToSlide(targetSlide);
                    // Close TOC after navigation
                    tocPanel.classList.remove('active');
                    tocIcon.style.display = 'flex';
                }
            });
        });

        // Initialize
        document.getElementById('totalSlides').textContent = totalSlides;
        updateSlide(1);

        // Render LaTeX math with KaTeX
        function renderMath() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "\\(", right: "\\)", display: false}
                ],
                throwOnError: false,
                errorColor: '#cc0000',
                strict: false
            });
        }
    </script>
</body>
</html>
