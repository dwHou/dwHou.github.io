<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>深度学习框架</title>

        <link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
        <link rel="stylesheet" href="../../fonts/Serif-Slanted/cmun-serif-slanted.css" />

        <!--BOOTSTRAP-->
        <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
        <!--mobile first-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <!--removed html from url but still is html-->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <!--font awesome-->
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

        <!--fonts: allan & cardo-->
        <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
        <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        <link href="../../css/sticky-footer-navbar.css" rel="stylesheet">

        <link href="../../css/default.css" rel="stylesheet">

        <link href="../../comments/inlineDisqussions.css" rel="stylesheet">

        <!--Highlight-->
        <link href="../../highlight/styles/github.css" rel="stylesheet">

        <link href="../../favicon.ico" rel="shortcut icon" />

        <!--<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
        <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <style>
        .post{width:170px;min-height:175px;padding-left:5px;padding-right:5px;float:left;border-left:1px solid #CCC;background-color:white;}
        div a:first-of-type .post { border-left: none; }
        .post:hover {filter: brightness(90%);}
        .post h3{margin:5px;font-size:75%;text-align:center}
        .post h4{margin:0px;font-size:50%;text-align:center}
        .post img{margin:0px;padding:2px;margin-bottom:10px;width:100%;height:155px}
        </style>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-49811703-1', 'dwHou.github.io');
          ga('require', 'linkid', 'linkid.js');
          ga('require', 'displayfeatures');
          ga('send', 'pageview');

        </script>

    </head>

    <body>
        <div id="wrap">
            <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
                <div class="container">
                    <!--Toggle header for mobile-->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand active" href="../../" style="font-size:20px;">De's blog</a>
                    </div>
                    <!--normal header-->
                    <div class="navbar-collapse collapse">
                        <ul class="nav navbar-nav navbar-right">
                            <li><a href="../../"><span class="glyphicon glyphicon-pencil"></span>  Blog</a></li>
                            <li><a href="../../about.html"><span class="glyphicon glyphicon-user"></span>  About</a></li>
                            <li><a href="../../contact.html"><span class="glyphicon glyphicon-envelope"></span>  Contact</a></li>
                            <li><a href="../../demo.html"><span class="glyphicon glyphicon-play"></span>  ALGO</a></li>
                        </ul>
                    </div><!--/.nav-collapse -->
                </div>
            </nav>


            <div id="content">
                <div class="container">
                    <div class="row">
                        <div class="col-md-8">
                            <h1>PyTorch</h1>
                            <div style="font-size: 170%;">持续更新</div>
                            <br>
                            <div class="info">
    <p style="font-family:CMSS; font-size:120%">Posted on Feb.19, 2020</p>

    <!--
        by dwHou
    -->
</div>
</br>


  <style>
    ul li {
      margin-top: 12px;
    }

    .tight-list li {
      margin-top: 6px;
    }
  </style>


  <h2>Pytorch常用代码段</h2>
  <ul>

  <li><a href="./PyTorch常用代码段.html"> 📓Cookbook </a></li>
    
  </ul>


  <h2>框架使用经验</h2>
  <ul>

  </ul>

  <h2>深度学习实验管理</h2>
  <style>
    .hovertext:hover {font-weight: lighter; color: #58ACFA; }
  </style>
  以一个开源<a class="hovertext" href="https://github.com/L1aoXingyu/Deep-Learning-Project-Template">codebase</a>为例，
  如果长期维护一个深度学习项目，代码的组织就比较重要了。如何设计一个简单而可扩展的结构是非常重要的。
  这就需要用到软件工程中的OOP设计。让我们高效、标准化地管理深度学习实验。
  
  <ul>
      <style>.hovertext:hover {font-weight: bold; color: #58ACFA; }</style>
      <li><b><a href="./实验管理.html" class="hovertext">跳转</a></b></li>
      <li>熟悉工具</li>
      <li>参数管理</li>
      <li>日志管理</li>
  </ul>
  

  <h2>遇到的坑/bug</h2>
  <ul>
      <li>in-place操作</li>
      in-place operation在pytorch中是指改变一个tensor的值的时候，不经过复制操作，而是直接在原来的内存上改变它的值。可以把它成为原地操作符。
      在pytorch中经常加后缀“_”来代表原地in-place operation，比如说.add_() 或者.scatter()。python里面的+=，*=也是in-place operation。
      PyTorch旧版本有提示：除非您在内存压力很大的情况下，否则您可能永远不需要使用它们。
      当时的原因是１．覆盖梯度计算所需的值。２．每个in-place操作实际上需要实现重写计算图。
      <pre>
          卷积这些操作不是in-place，
          relu可选in-place，
          -=,+=这类是in-place，
          CAIN实现的改版的meanshift——sub_mean()就也是in-place的。
          因为你进入网络就进行了原地操作，所以prediction = model(input)会改动到input.
          解决方案就是，进入网络后
          foward(x):
                x = x.clone()
          RCAN、EDSR这些DIV2k数据集上的Meanshift是用卷积方式实现的，就不存在这个问题。

          <font color="red">总结:</font> 不想受到in-place影响的变量，可以拷贝x.copy()，或者如果是张量可以x.clone()。
      </pre>

      <li>检测inplace异常</li>
      <pre>
          # detect inplace-error
          torch.autograd.set_detect_anomaly(True)
      </pre>

      <li>初始化</li>
      <pre>PyTorch Module自带的默认初始化方法往往更可靠。</pre>
      <li>数据集输入问题</li>
      <pre>把训练集dataloader的shuffle关掉，溯源找到数据集对应元素。例如优酷超分数据集就有2048x1152分辨率的视频。
           要记得数据集是有shuffle的，在处理数据逻辑时一定要意识到。
      </pre>
      <li>模型验证集挺好的，结果测试色偏、失真特别奇怪</li>
      <pre>很有可能就是忘了model.load_state_dict()，是按默认初始化跑的图片。psnr直接会降到十几二十。</pre>
      <li>PyTorch测试模型执行计算耗费的时间</li>
      <pre>
          <font color="#D2B4DE">一般我们都会使用这种方式一测试时间</font>

          # 方式一:
          star = time.time()
          result = model(input)
          end = time.time()

          <font color="#D2B4DE">但正确的应该是下边这种方式二</font>

          # 方式二:
          torch.cuda.synchronize()
          start = time.time()
          result = model(input)
          torch.cuda.synchronize()
          end = time.time()

          <font color="#D2B4DE">为什么这样呢？
          在pytorch里面，程序的执行都是异步的。如果采用第一种方式，测试的时间会很短，因为执行完end=time.time()程序就退出了，后台的cu也因为python的退出退出了；
              如果采用第二种方式，代码会同步cu的操作，等待gpu上的操作都完成了再继续计算end = time.time()</font>

          <font color="#D2B4DE">如果将方式一代码改为方式三：</font>

          # 方式三:
          start = time.time()
          result = model(input)
          print(result) #或 result.cpu()
          end = time.time()

          <font color="#D2B4DE">这时候会发现方式三和方式二的时间是类似的，因为方式三会等待gpu上的结果执行完传给print函数，所以此时间就和方式二同步操作的时间基本上是一致的了。
              将print(result)换成result.cpu()也可以得到相同的结果。</font>


          <font color="#D2B4DE">来自作者：几时见得清梦 的简书笔记</font>

      </pre>
      <li>torch.cuda.synchronize()</li>
        等待当前设备上所有流中的所有核心完成。 如果不加syn, forward会马上返回。
        但加上syn后，cpu会等待模型实际运行完再获取数据.实际使用时，如果不逐段统计时间，可以不加这个sync.
        <pre>

            #<font color="#D4E6F1"></font>
            <font color="#2E86C1">
            from contextlib import contextmanager

            @contextmanager
            def timer(self, name):
                start = torch.cuda.Event(enable_timing=True)
                end = torch.cuda.Event(enable_timing=True)

                start.record()
                yield
                end.record()

                torch.cuda.synchronize()
                print(f'[{name}] done in {start.elapsed_time(end):.3f} ms')

            with timer('GTX 1080 Ti 720p inference'):
                    out = model(input)
            </font>
            这样结果才一致的惹。
        </pre>
      <li>对比试验的优化器和lr更新策略</li>
      <pre>
①不使用scheduler, Adam + 一个大的epoch训下去。这样比较公平。
②ReduceLROnPlateau有点运气成分，要么就在loss是否下降上做。如果是验证集PSNR，那最好将容忍度设很大。
③余弦退火。策略一致就是可以的。
      </pre>

      <li>train 与 val 的区别</li>
      <pre>
          model.train() ：启用 BatchNormalization 和 Dropout
          model.eval() ：不启用 BatchNormalization 和 Dropout
          val时会加上 with torch.no_grad()
      </pre>

      <li>model.val() 的注意事项</li>
      <pre>
首先看看官方<a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?highlight=batchnorm#torch.nn.BatchNorm2d">Doc</a>接口
<a href="https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561/2">Model.eval() gives incorrect loss for model with batchnorm layers</a>
有人说set the track_running_stats=False for all batch norm layers in the model.
for child in model.children():
    for ii in range(len(child)):
        if type(child[ii])==nn.BatchNorm2d:
            child[ii].track_running_stats = False
有人说这似乎就是回到了不用model.eval()，uses the current batch’s
mean and variance to do the normalization，我认为这就相当于momentum=0。不受历史的影响。
<b>而出现这个情况的原因，就是数据集不适合用batchnorm，比如太多noisy了(我任务确实如此)，batchsize太小了(DDP时确实)，分布相差太大啦，
所以这里要把momentum设小，设置为0，即track_running_stats = False。</b>
但也有个官方人士建议设大的，这样更能规避异常值(non-stationary)，维持之前学习到的。

个人认为要深刻理解，灵活应用，再想一想。
<b>
来，我们从原理说起。本来BatchNorm是回到正太分布，这样对训练有许多帮助。
但是呢，怎么回到正太分布呢，你要知道缩放weight和平移量bias。
任何框架下的任何形式的Normalization都是如此，这个缩放和平移的值是可学习、可训练的参数。
我们通过这两个参数，能把特征转到'正太'上。但是！我们不可能，也没想要完全严格的正太分布。
①来，weight和bias是根据统计特性学习来的，对于特定的特征它也转不到正太。
②来，若是真的全为正太了，还有啥区分度，没有可用信息了。
所以我们的实际目的，只是控制均值和方差的大小。而不是分要它们严格等于多少。

然后momentum小的话，可以让weight和bias的得出多根据当前的输入，少累计历史的影响。
我认为这样是防止在历史上的"过拟合"，实则历史信息对更好接近正太帮助不大。适合我们在分布经常变化的情况下使用。
track_running_stats=False也是这个作用，只利用当前batch的统计信息，没有历史buffer。

而momentum大的话，则更依赖历史buffer里的信息。这样说明历史信息的作用是比当前统计信息帮助大的。

综上，去高斯噪声这样的、high-level这样的，特征其实分布很有特点，变化不大。是适合用batchnorm，并且momentum大一点。
至于对齐、光流这样的，momentum小一点保险。超分辨率，不用batchnorm更好。
用或者不用都好说，最不希望的是训练时其他模块配合batchnorm"过拟合"到正太，但你推理时batchnorm又效果不好，到不了正太。

至于model.eval()对其的影响，
During training, this layer keeps a running estimate of its computed mean and variance.
The running sum is kept with a default momentum of 0.1.

During evaluation, this running mean/variance is used for normalization.
意思就是eval()时，才把所有历史信息拿出来用。这些都是训练(running)时按momentum来存着的。
为什么要开启它呢，认为历史信息有帮助。
这叫做<font color="red">"贝塞尔校正"</font>
I mean, why is it better to use model.eval and
take the running statistics and not rely on the current test image statistics?
那为什么以为有帮助的eval()还导致性能下降呢。就是历史信息其实起了负面影响。
但有时eval()是有帮助的，会提升效果。你可以通过同样的checkpoint，推理时改变track_running_stats来判断。
最后<font color="red">择优</font>。
if you compute test stat, then you are basically “train” on test set,
because stat in this case is a trained param.
You can do it, nobody says you can’t, just that ppl would consider it “cheating”
但还有很重要的一点！一定要测试时开着eval()，不然相当于数据"泄露"。测试集在训练时留了痕迹。
这样的测试是不公正的。
这是为什么用eval()的原因。可以防止"在测试集过拟合"。至于用它反而性能下降就是上面说的历史信息反而不好。

结论：我们一定要推理用eval()。至于momentum、track_running_stats到底怎么用，就不一而足了，不是原则问题，
怎么效果好怎么来。我的经验是，历史信息一般是有用的。一般来说track_running_stats开着会有帮助。
历史信息什么情况越来越没用呢，甚至负面作用呢，就是分布变化大、batchsize小(也使得分布波动大)的时候，
历史与当前的Gap太大，就不好了。
至于那些eval()之后性能还下降的，首先它们一开始数据泄露了，不值得提倡。其次track_running_stats=False罢，历史信息负面帮助。
终究track_running_stats=True or False对比便知, 或者momentum=0来试, <font color="red">择优</font>。

我觉得最好就是，训练完成后，再大batch的只forward，不backward，再保存模型。让历史信息充分发挥作用！！！
</b>

但也有人说这其实不是不对的。
<font color="red">我的观点，或许这和任务以及你的训练batch大小有关，如果任务不讲究batch，
训练时batch又小，本来就不稳定，或许不需要加model.eval()了。</font>

Batch Normalization里有一个momentum参数, 该参数作用于mean和variance的计算上,
这里保留了历史batch里的mean和variance值,即 moving_mean和moving_variance,
借鉴优化算法里的momentum算法将历史batch里的mean和variance的作用延续到当前batch.
一般momentum的值为0.9 , 0.99等. 多个batch后, 即多个0.9连乘后,最早的batch的影响会变弱.

所以也有建议，在遇到non-stationary training时，<font color="red">把momentum设小一些。</font>
def evaluate_batch(net, batch, output, shouldeval):
        if shouldeval:
            net.eval()
            net.bn1.train()
            # 我感觉这更要绝，不仅不用eval()了，还不留任何历史影响。是否矫枉过正了，我不用eval就是了。
            net.bn1.momentum = 0.0
        else:
            net.train()
...
        #before returning
        net.bn1.momentum = 0.1

下面这段是我看到最清晰直接、讲明白原理本质的解释了。
「The high validation loss is due to the wrong estimates of the running stats.
Since you are feeding a constant tensor (batchone: mean=1, std=0) and a random tensor (batchtwo: mean~=0, std~=1), the running estimates will be shaky and wrong for both inputs.

During training the current batch stats will be used to compute the output, so that the model might converge.
However, during evaluation the batchnorm layer tries to normalize both inputs with skewed running estimates, which yields the high loss values.
Usually we assume that all inputs are from the same domain and thus have approx. the same statistics.

If you set track_running_stats=False in your BatchNorm layer, the batch statistics will also be used during evaluation, which will reduce the eval loss significantly.」


If you turn track_running_stats off (as suggested in the post) you will instead
use the mean and std of the batch in eval mode. This is flawed and incorrect usage,
since you will get an inference result which is based on the data in your batch.
所以提出这个观点的人认为就不要用batchnorm了，而是用groupnorm。

很多人遇到这个问题无计可施，也都提到不用model.eval()就没问题。
I tried:
・change the momentum term in BatchNorm constructor to higher.
・before you set model.eval() , run a few inputs through model (just forward pass, you dont need to backward). This will help stabilize the running_mean / running_std values.
・increase Batchsize
Nothing helped. 不过这人发现自己是在不同地方用了同样的batchnorm.
In the end I saw I was indeed using the same BatchNorm layers in different parts of the network.
Once I changed that it worked again.

      </pre>

      <li> torch.nn 与 torch.nn.functional</li>
      <pre>

      </pre>

      <li>加载模型接着训练，Adam动量对齐</li>
      <pre>
如果使用了Adam，应该保存optimizer state dict，以便继续训练时加载它。
      </pre>

      <li> Metrics异常 </li>
      <pre>
如果LPIPS与PSNR、SSIM的得分明显不成正比时，PSNR异常低的那组数据考虑检查是不是帧没对上。有次同学犯了个好玩的bug，就是把EDVR弄成了input(0,1,2,3,4)，超分3号帧。
结果测试时自然都会移动一格。
没对上的话，PSNR肯定低，但对于LPIPS这样的指标影响则不大。
      </pre>

      <li>PyTorch1.6训练保存的模型在1.4低版本无法加载</li>
      <pre>
在1.6:  torch.save(model_.state_dict(), 'model_best_bacc.pth.tar', _use_new_zipfile_serialization=False)
https://github.com/pytorch/pytorch/issues/48915
注： 这种方式转换过的模型，字典关键词会移除多卡的标识'module'
      </pre>

      <li>模型和保存点不匹配</li>
      <pre>
      在排除了GPU/CPU和单卡多卡的问题之后，怀疑到是代码变动。
          state_dict = torch.load(arg.ckp)
          from collections import OrderedDict
          new_state_dict = OrderedDict()
          for k, v in state_dict.items():
              name = k[7:] # remove 'module'.
              new_state_dict[name] = v
          print(new_state_dict.keys())
      将结果输出到 > module.txt看，宽度缩小到单栏，然后顺着网络层排查是哪儿的代码没对上。
      或
      for k in state_dict.keys():
        print(k)

      def print_network(net):
        num_params = 0
        for param in net.parameters():
            num_params += param.numel()
        print(net)
        print('Total number of parameters: %d' % num_params)
      </pre>

      <li>层层检查shape变化</li>
      <pre>
          image = torch.zeros((1, 3, 64, 64))
          out = image
          for name, op in resnet18.items():
            out = op(out)
            print(name, out.shape)
          不过这需要定义和foward出现的先后顺序一致。
      </pre>

      <li>显存不够用，不满足大patch & 合适的batch size</li>
      <pre>
          梯度累加
          for i,(features,target) in enumerate(train_loader):
            outputs = model(images)  # 前向传播
            loss = criterion(outputs,target)  # 计算损失
            loss = loss/accumulation_steps   # 可选，如果损失要在训练样本上取平均

            loss.backward()  # 计算梯度
            if((i+1)%accumulation_steps)==0:
                optimizer.step()        # 反向传播，更新网络参数
                optimizer.zero_grad()   # 清空梯度

          不过bn层会受到点影响，可通过调小momentum参数解决。
          https://www.zhihu.com/question/303070254
      </pre>

  </ul>



<!-- 加载出评论，是使用Disqus的论坛短名（shortname）
A shortname is the unique identifier assigned to a Disqus site. 

https://segmentfault.com/a/1190000005773009
https://help.disqus.com/en/articles/1717111-what-s-a-shortname
https://blog.csdn.net/weixin_34327761/article/details/89630337
   -->

<div id="disqus_thread"></div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../comments/inlineDisqussions.js"></script>
<script src="../../js/disqus.js"></script>

                        </div>
                        <div class="col-md-4"></div>
                    </div>
                </div>
            </div>


            <div id="footer">
                <div class="container">
                    Built by <a href="https://github.com/oinkina">Oinkina</a> with
                    <a href="http://jaspervdj.be/hakyll">Hakyll</a>
                    using <a href="http://getbootstrap.com/">Bootstrap</a>,
                    <a href="http://www.mathjax.org/">MathJax</a>,
                    <a href="http://disqus.com/">Disqus</a>,
                    <a href="https://github.com/unconed/MathBox.js">MathBox.js</a>,
                    <a href="http://highlightjs.org/">Highlight.js</a>,
                    and <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>.
                </div>
            </div>
        </div>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

    <script src="../../bootstrap/js/bootstrap.min.js"></script>

    <script src="../../highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script src="../../js/footnotes.js"></script>

    <script src="../../comments/inlineDisqussions.js"></script>

    <noscript>Enable JavaScript for footnotes, Disqus comments, and other cool stuff.</noscript>

    </body>

</html>
