# 机器学习编译

[课程材料](https://mlc.ai/zh/index.html)|[视频](https://space.bilibili.com/1663273796/video)

🐮[LLVM,MLIR, Chris Lattner](https://nondot.org/sabre/)；[TVM, Tianqi Chen](https://tqchen.com/)；[Halide, MIT](https://halide-lang.org/);

一般要部署模型到一个指定设备上，我们一般会使用硬件厂商自己推出的一些前向推理框架，例如在Intel的CPU/GPU上就使用OpenVINO，在Arm的CPU/GPU上使用NCNN/MNN等，在Nvidia GPU上使用TensorRT。虽然针对不同的硬件设备我们使用特定的推理框架进行部署是最优的，但这也同时存在问题，比如一个开发者训练了一个模型需要在多个不同类型的设备上进行部署，那么开发者需要将训练的模型分别转换到特定框架可以读取的格式，并且还要考虑各个推理框架OP实现是否完全对齐的问题，然后在不同平台部署时还容易出现的问题是开发者训练的模型在一个硬件上可以高效推理，部署到另外一个硬件上性能骤降。并且从之前几篇探索ONNX的文章来看，不同框架间模型转换工作也是阻碍各种训练框架模型快速落地的一大原因。

接下来，我们要简单描述一下编译器。实际上在编译器发展的早期也和要将**各种深度学习训练框架的模型部署到各种硬件**面临的情况一下，历史上出现了非常多的编程语言，比如C/C++/Java等等，然后每一种硬件对应了一门特定的编程语言，再通过特定的编译器去进行编译产生机器码，可以想象随着硬件和语言的增多，编译器的维护难度是多么困难。还好现代的编译器已经解决了这个问题，那么这个问题编译器具体是怎么解决的呢？

为了解决上面的问题，科学家为编译器抽象出了编译器前端，编译器中端，编译器后端等概念，并引入IR (Intermediate Representation)的概率。解释如下：

- 编译器前端：接收C/C++/Java等不同语言，进行代码生成，吐出IR
- 编译器中端：接收IR，进行不同编译器后端可以共享的优化，如常量替换，死代码消除，循环优化等，吐出优化后的IR
- 编译器后端：接收优化后的IR，进行不同硬件的平台相关优化与硬件指令生成，吐出目标文件

致谢以下作者的知识分享：蓝色，BBuf，高洋，立交桥跳水冠军，尤洋，陈天奇，Oldpan，Hongzheng Chen「陈鸿峥」



## 01 机器学习编译概述